<html lang="en"><head><meta charset="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Build models · NLP with R and UDPipe</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta property="og:title" content="Build models · NLP with R and UDPipe"/><meta property="og:type" content="website"/><meta property="og:url" content="https://bnosac.github.io/udpipe/index.html"/><meta property="og:description" content="## General"/><link rel="shortcut icon" href="/udpipe/img/favicon.ico"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css"/><link rel="alternate" type="application/atom+xml" href="https://bnosac.github.io/blog/atom.xml" title="NLP with R and UDPipe Blog ATOM Feed"/><link rel="alternate" type="application/rss+xml" href="https://bnosac.github.io/blog/feed.xml" title="NLP with R and UDPipe Blog RSS Feed"/><script type="text/javascript" src="https://buttons.github.io/buttons.js"></script><link rel="stylesheet" href="/udpipe/css/main.css"/></head><body class="sideNavVisible"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/udpipe/en"><img class="logo" src="/udpipe/img/logo-udpipe-r.png"/></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li><a href="https://bnosac.github.io/udpipe/en/index.html" target="_self">Home</a></li><li><a href="/udpipe/docs/doc0.html" target="_self">Docs</a></li><li><a href="https://bnosac.github.io/udpipe/en/help.html" target="_self">Support</a></li><li><a href="https://github.com/bnosac/udpipe" target="_self">GitHub</a></li><li><a href="/udpipe/blog" target="_self">Blog</a></li><li><a target="_self"></a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="container docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><i></i></div><h2><i>›</i><span>Annotation</span></h2></div><div class="navGroups"><div class="navGroup navGroupActive"><h3>Introduction</h3><ul><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc0.html">Try it out</a></li><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc1.html">Introduction</a></li></ul></div><div class="navGroup navGroupActive"><h3>Annotation</h3><ul><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc2.html">Text Annotation</a></li><li class="navListItem navListItemActive"><a class="navItem navItemActive" href="/udpipe/docs/doc3.html">Model building</a></li><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc8.html">Parallel Annotation</a></li></ul></div><div class="navGroup navGroupActive"><h3>Analytical use cases</h3><ul><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc5.html">Use Cases I</a></li><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc6.html">Use Cases II</a></li><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc7.html">Use Cases III</a></li></ul></div><div class="navGroup navGroupActive"><h3>MoreDocs</h3><ul><li class="navListItem"><a class="navItem" href="/udpipe/docs/doc4.html">More docs</a></li></ul></div></div></section></div><script>
          var toggler = document.getElementById('navToggler');
          var nav = document.getElementById('docsNav');
          toggler.onclick = function() {
            nav.classList.toggle('docsSliderActive');
          };
        </script></nav></div><div class="container mainContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1>Build models</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" name="general"></a><a href="#general" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>General</h2>
<p>This package vignette shows how to build your own text annotation models based on UDPipe, allowing you to have full control over how you like that the model will execute: Tokenization (1), Parts of Speech tagging (2), Lemmatization (3) and Dependency Parsing (4).</p>
<p>This section is also relevant if you work in a commercial setting where you would like to build and use your own models to annotate text. Note that some pre-trained models which you can download with <code>udpipe_download_model</code> were released under the CC-BY-NC-SA license, others were released under the CC-BY-SA license, the latter allowing for more liberal use. Mark that if you want to see how these models have been built, you can take inspiration from the training code which is available at <a href="https://github.com/bnosac/udpipe.models.ud">https://github.com/bnosac/udpipe.models.ud</a>.</p>
<p>In order to train annotation models, you need to have data in <strong>CONLL-U format</strong>, a format which is described at <a href="http://universaldependencies.org/format.html">http://universaldependencies.org/format.html</a>. At the time of writing this, for more than 60 languages, open treebanks in CONLL-U format are made available for download at <a href="http://universaldependencies.org/#ud-treebanks">http://universaldependencies.org/#ud-treebanks</a>. Most of these treebanks are distributed under the CC-BY-SA license which allows commercial use.</p>
<p>Mark that if you will build your own models, you will probably be interested in reading the paper with the details of the techniques used by UDPipe: &quot;Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe&quot;, available at <a href="http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf">http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf</a>.</p>
<h2><a class="anchor" aria-hidden="true" name="model-building"></a><a href="#model-building" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Model building</h2>
<h3><a class="anchor" aria-hidden="true" name="basic-example"></a><a href="#basic-example" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Basic example</h3>
<p>Currently the package allows you to fit a text annotation model by using the function <code>udpipe_train</code>. You have to give it a character vector of files which are in CONLL-U format (which you might have downloaded at <a href="http://universaldependencies.org/#ud-treebanks">http://universaldependencies.org/#ud-treebanks</a>).</p>
<p>Such at file basically looks like this, or has a similar format. You can just download these from <a href="http://universaldependencies.org">http://universaldependencies.org</a> for the language of your choice.</p>
<pre><code class="hljs css r">file_conllu &lt;- system.file(package = <span class="hljs-string">"udpipe"</span>, <span class="hljs-string">"dummydata"</span>, <span class="hljs-string">"traindata.conllu"</span>)
file_conllu
</code></pre>
<pre><code class="hljs">[<span class="hljs-number">1</span>] <span class="hljs-string">"C:/Users/Jan/Documents/R/win-library/3.4/udpipe/dummydata/traindata.conllu"</span>
</code></pre>
<pre><code class="hljs css r">cat(head(readLines(file_conllu), <span class="hljs-number">3</span>), sep=<span class="hljs-string">"\n"</span>)
</code></pre>
<pre><code class="hljs"><span class="hljs-meta">#</span><span class="bash"> newdoc id = doc1</span>
<span class="hljs-meta">#</span><span class="bash"> newpar</span>
<span class="hljs-meta">#</span><span class="bash"> sent_id = 1</span>
</code></pre>
<p>If you have this type of file and you provide it to <code>udpipe_train</code>, a model is saved on disk in a binary format which can then be used to annotate your text data using <code>udpipe_annotate</code> in order to Tokenize, get Parts of Speech tags, find Lemma's or to extract Dependency relationships.
Let's show how this training works on the toy CONLL-U file we just showed:</p>
<pre><code class="hljs css r"><span class="hljs-keyword">library</span>(udpipe)
m &lt;- udpipe_train(file = <span class="hljs-string">"toymodel.udpipe"</span>, files_conllu_training = file_conllu, 
                  annotation_tokenizer = list(dimension = <span class="hljs-number">16</span>, 
                                              epochs = <span class="hljs-number">1</span>, 
                                              batch_size = <span class="hljs-number">100</span>, 
                                              dropout = <span class="hljs-number">0.7</span>),
                  annotation_tagger = list(iterations = <span class="hljs-number">1</span>, 
                                           models = <span class="hljs-number">1</span>, 
                                           provide_xpostag = <span class="hljs-number">1</span>, 
                                           provide_lemma = <span class="hljs-number">0</span>, 
                                           provide_feats = <span class="hljs-number">0</span>), 
                  annotation_parser = <span class="hljs-string">"none"</span>)
</code></pre>
<pre><code class="hljs">Training tokenizer with the following options: <span class="hljs-attribute">tokenize_url</span>=1, <span class="hljs-attribute">allow_spaces</span>=0, <span class="hljs-attribute">dimension</span>=16
  <span class="hljs-attribute">epochs</span>=1, <span class="hljs-attribute">batch_size</span>=100, <span class="hljs-attribute">learning_rate</span>=0.0050, <span class="hljs-attribute">dropout</span>=0.7000, <span class="hljs-attribute">early_stopping</span>=0
Epoch 1, logprob: -2.1721e+005, training acc: 84.20%
Tagger model 1 columns: lemma <span class="hljs-attribute">use</span>=1/provide=0, xpostag <span class="hljs-attribute">use</span>=1/provide=1, feats <span class="hljs-attribute">use</span>=1/provide=0
Creating morphological dictionary <span class="hljs-keyword">for</span> tagger model 1.
Tagger model 1 dictionary options: <span class="hljs-attribute">max_form_analyses</span>=0, custom <span class="hljs-attribute">dictionary_file</span>=none
Tagger model 1 guesser options: <span class="hljs-attribute">suffix_rules</span>=8, <span class="hljs-attribute">prefixes_max</span>=0, <span class="hljs-attribute">prefix_min_count</span>=10, <span class="hljs-attribute">enrich_dictionary</span>=6
Tagger model 1 options: <span class="hljs-attribute">iterations</span>=1, <span class="hljs-attribute">early_stopping</span>=0, <span class="hljs-attribute">templates</span>=tagger
Training tagger model 1.
Iteration 1: done, accuracy 44.44%
</code></pre>
<pre><code class="hljs css r">m$file_model
</code></pre>
<pre><code class="hljs">[<span class="hljs-number">1</span>] <span class="hljs-string">"toymodel.udpipe"</span>
</code></pre>
<pre><code class="hljs css r"><span class="hljs-comment">## The model is now trained and saved in file toymodel.udpipe in the current working directory</span>
<span class="hljs-comment">## Now we can use the model to annotate some text</span>
mymodel &lt;- udpipe_load_model(<span class="hljs-string">"toymodel.udpipe"</span>)
x &lt;- udpipe_annotate(
  object = mymodel, 
  x = <span class="hljs-string">"Dit is een tokenizer met POS tagging, 
       zonder lemmatisation noch laat deze dependency parsing toe."</span>, 
  parser = <span class="hljs-string">"none"</span>)
str(as.data.frame(x))
</code></pre>
<pre><code class="hljs"><span class="hljs-string">'data.frame'</span>:   <span class="hljs-number">15</span> obs. of  <span class="hljs-number">14</span> variables:
 $ doc_id       : chr  <span class="hljs-string">"doc1"</span> <span class="hljs-string">"doc1"</span> <span class="hljs-string">"doc1"</span> <span class="hljs-string">"doc1"</span> <span class="hljs-keyword">...</span>
 $ paragraph_id : int  <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-keyword">...</span>
 $ sentence_id  : int  <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> <span class="hljs-keyword">...</span>
 $ sentence     : chr  <span class="hljs-string">"Dit is een tokenizer met POS tagging, zonder lemmatisation noch laat deze dependency parsing toe."</span> <span class="hljs-string">"Dit is een tokenizer met POS tagging, zonder lemmatisation noch laat deze dependency parsing toe."</span> <span class="hljs-string">"Dit is een tokenizer met POS tagging, zonder lemmatisation noch laat deze dependency parsing toe."</span> <span class="hljs-string">"Dit is een tokenizer met POS tagging, zonder lemmatisation noch laat deze dependency parsing toe."</span> <span class="hljs-keyword">...</span>
 $ token_id     : chr  <span class="hljs-string">"1"</span> <span class="hljs-string">"2"</span> <span class="hljs-string">"3"</span> <span class="hljs-string">"4"</span> <span class="hljs-keyword">...</span>
 $ token        : chr  <span class="hljs-string">"Dit"</span> <span class="hljs-string">"is"</span> <span class="hljs-string">"een"</span> <span class="hljs-string">"tokenizer"</span> <span class="hljs-keyword">...</span>
 $ lemma        : chr  <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-keyword">...</span>
 $ upos         : chr  <span class="hljs-string">"PRON"</span> <span class="hljs-string">"VERB"</span> <span class="hljs-string">"AUX"</span> <span class="hljs-string">"NOUN"</span> <span class="hljs-keyword">...</span>
 $ xpos         : chr  <span class="hljs-string">"Pron|onbep|neut|zelfst"</span> <span class="hljs-string">"V|intrans|ott|3|ev"</span> <span class="hljs-string">"V|hulpofkopp|ott|1|ev"</span> <span class="hljs-string">"N|soort|ev|neut"</span> <span class="hljs-keyword">...</span>
 $ feats        : chr  <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-keyword">...</span>
 $ head_token_id: chr  <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-keyword">...</span>
 $ dep_rel      : chr  <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-keyword">...</span>
 $ deps         : chr  <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-keyword">...</span>
 $ misc         : chr  <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-literal">NA</span> <span class="hljs-keyword">...</span>
</code></pre>
<p>In the above example, we trained only a tokenizer and POS tagger, excluding lemmatisation and feature tagging and without dependency parsing. This was done by setting the <code>annotation_parser</code> argument to 'none' and setting <code>provide_lemma</code> and <code>provide_feats</code> to 0. The other arguments were merely set to reduce computation time in this package vignette.</p>
<h3><a class="anchor" aria-hidden="true" name="providing-more-details-on-the-model-annotation-process"></a><a href="#providing-more-details-on-the-model-annotation-process" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Providing more details on the model annotation process</h3>
<p>If you want to create a tagger which is capable of executing tokenisation, tagging as well as dependency parsing with the default settings of the algorithm, you just proceed as follows.</p>
<pre><code class="hljs css r">m &lt;- udpipe_train(file = <span class="hljs-string">"toymodel.udpipe"</span>, files_conllu_training = file_conllu, 
                  annotation_tokenizer = <span class="hljs-string">"default"</span>,
                  annotation_tagger = <span class="hljs-string">"default"</span>,
                  annotation_parser = <span class="hljs-string">"default"</span>)
</code></pre>
<p>When you want to train the model with specific tokenizer/tagger/parser settings, you need to provide these settings as a list to the respective arguments <code>annotation_tokenizer</code>, <code>annotation_tagger</code> and <code>annotation_parser</code>. The possible options for each of these settings are explained in detail below and their logic is detailed in the paper &quot;Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe&quot;, available at <a href="http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf">http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf</a>.</p>
<pre><code class="hljs css r">params &lt;- list()

<span class="hljs-comment">## Tokenizer training parameters</span>
params$tokenizer &lt;- list(dimension = <span class="hljs-number">24</span>, 
                         epochs = <span class="hljs-number">1</span>, <span class="hljs-comment">#epochs = 100, </span>
                         initialization_range = <span class="hljs-number">0.1</span>, 
                         batch_size = <span class="hljs-number">100</span>, learning_rate = <span class="hljs-number">0.005</span>, 
                         dropout = <span class="hljs-number">0.1</span>, early_stopping = <span class="hljs-number">1</span>)

<span class="hljs-comment">## Tagger training parameters</span>
params$tagger &lt;- list(models = <span class="hljs-number">2</span>, 
  templates_1 = <span class="hljs-string">"tagger"</span>, 
      guesser_suffix_rules_1 = <span class="hljs-number">8</span>, guesser_enrich_dictionary_1 = <span class="hljs-number">6</span>, 
      guesser_prefixes_max_1 = <span class="hljs-number">0</span>, 
      use_lemma_1 = <span class="hljs-number">0</span>, use_xpostag_1 = <span class="hljs-number">1</span>, use_feats_1 = <span class="hljs-number">1</span>, 
      provide_lemma_1 = <span class="hljs-number">0</span>, provide_xpostag_1 = <span class="hljs-number">1</span>, 
      provide_feats_1 = <span class="hljs-number">1</span>, prune_features_1 = <span class="hljs-number">0</span>, 
  templates_2 = <span class="hljs-string">"lemmatizer"</span>, 
      guesser_suffix_rules_2 = <span class="hljs-number">6</span>, guesser_enrich_dictionary_2 = <span class="hljs-number">4</span>, 
      guesser_prefixes_max_2 = <span class="hljs-number">4</span>, 
      use_lemma_2 = <span class="hljs-number">1</span>, use_xpostag_2 = <span class="hljs-number">0</span>, use_feats_2 = <span class="hljs-number">0</span>, 
      provide_lemma_2 = <span class="hljs-number">1</span>, provide_xpostag_2 = <span class="hljs-number">0</span>, 
      provide_feats_2 = <span class="hljs-number">0</span>, prune_features_2 = <span class="hljs-number">0</span>)

<span class="hljs-comment">## Dependency parser training parameters</span>
params$parser &lt;- list(iterations = <span class="hljs-number">1</span>, 
  <span class="hljs-comment">#iterations = 30, </span>
  embedding_upostag = <span class="hljs-number">20</span>, embedding_feats = <span class="hljs-number">20</span>, embedding_xpostag = <span class="hljs-number">0</span>, 
  embedding_form = <span class="hljs-number">50</span>, 
  <span class="hljs-comment">#embedding_form_file = "../ud-2.0-embeddings/nl.skip.forms.50.vectors", </span>
  embedding_lemma = <span class="hljs-number">0</span>, embedding_deprel = <span class="hljs-number">20</span>, 
  learning_rate = <span class="hljs-number">0.01</span>, learning_rate_final = <span class="hljs-number">0.001</span>, l2 = <span class="hljs-number">0.5</span>, hidden_layer = <span class="hljs-number">200</span>, 
  batch_size = <span class="hljs-number">10</span>, transition_system = <span class="hljs-string">"projective"</span>, transition_oracle = <span class="hljs-string">"dynamic"</span>, 
  structured_interval = <span class="hljs-number">10</span>)

<span class="hljs-comment">## Train the model</span>
m &lt;- udpipe_train(file = <span class="hljs-string">"toymodel.udpipe"</span>, 
                  files_conllu_training = file_conllu, 
                  annotation_tokenizer = params$tokenizer,
                  annotation_tagger = params$tagger,
                  annotation_parser = params$parser)
</code></pre>
<pre><code class="hljs">Training tokenizer <span class="hljs-keyword">with</span> the following options: <span class="hljs-attr">tokenize_url=1,</span> <span class="hljs-attr">allow_spaces=0,</span> <span class="hljs-attr">dimension=24</span>
  <span class="hljs-attr">epochs=1,</span> <span class="hljs-attr">batch_size=100,</span> <span class="hljs-attr">learning_rate=0.0050,</span> <span class="hljs-attr">dropout=0.1000,</span> <span class="hljs-attr">early_stopping=1</span>
Epoch <span class="hljs-number">1</span>, logprob: -<span class="hljs-number">1.4653</span>e+<span class="hljs-number">005</span>, training acc: <span class="hljs-number">89.57</span>%
Tagger model <span class="hljs-number">1</span> columns: lemma <span class="hljs-attr">use=0/provide=0,</span> xpostag <span class="hljs-attr">use=1/provide=1,</span> feats <span class="hljs-attr">use=1/provide=1</span>
Creating morphological dictionary for tagger model <span class="hljs-number">1</span>.
Tagger model <span class="hljs-number">1</span> dictionary options: <span class="hljs-attr">max_form_analyses=0,</span> custom <span class="hljs-attr">dictionary_file=none</span>
Tagger model <span class="hljs-number">1</span> guesser options: <span class="hljs-attr">suffix_rules=8,</span> <span class="hljs-attr">prefixes_max=0,</span> <span class="hljs-attr">prefix_min_count=10,</span> <span class="hljs-attr">enrich_dictionary=6</span>
Tagger model <span class="hljs-number">1</span> options: <span class="hljs-attr">iterations=20,</span> <span class="hljs-attr">early_stopping=0,</span> <span class="hljs-attr">templates=tagger</span>
Training tagger model <span class="hljs-number">1</span>.
Iteration <span class="hljs-number">1</span>: done, accuracy <span class="hljs-number">37.04</span>%
Iteration <span class="hljs-number">2</span>: done, accuracy <span class="hljs-number">81.48</span>%
Iteration <span class="hljs-number">3</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">4</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">5</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">6</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">7</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">8</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">9</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">10</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">11</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">12</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">13</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">14</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">15</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">16</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">17</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">18</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">19</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">20</span>: done, accuracy <span class="hljs-number">100.00</span>%
Tagger model <span class="hljs-number">2</span> columns: lemma <span class="hljs-attr">use=1/provide=1,</span> xpostag <span class="hljs-attr">use=0/provide=0,</span> feats <span class="hljs-attr">use=0/provide=0</span>
Creating morphological dictionary for tagger model <span class="hljs-number">2</span>.
Tagger model <span class="hljs-number">2</span> dictionary options: <span class="hljs-attr">max_form_analyses=0,</span> custom <span class="hljs-attr">dictionary_file=none</span>
Tagger model <span class="hljs-number">2</span> guesser options: <span class="hljs-attr">suffix_rules=6,</span> <span class="hljs-attr">prefixes_max=4,</span> <span class="hljs-attr">prefix_min_count=10,</span> <span class="hljs-attr">enrich_dictionary=4</span>
Tagger model <span class="hljs-number">2</span> options: <span class="hljs-attr">iterations=20,</span> <span class="hljs-attr">early_stopping=0,</span> <span class="hljs-attr">templates=lemmatizer</span>
Training tagger model <span class="hljs-number">2</span>.
Iteration <span class="hljs-number">1</span>: done, accuracy <span class="hljs-number">48.15</span>%
Iteration <span class="hljs-number">2</span>: done, accuracy <span class="hljs-number">77.78</span>%
Iteration <span class="hljs-number">3</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">4</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">5</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">6</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">7</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">8</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">9</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">10</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">11</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">12</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">13</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">14</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">15</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">16</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">17</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">18</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">19</span>: done, accuracy <span class="hljs-number">100.00</span>%
Iteration <span class="hljs-number">20</span>: done, accuracy <span class="hljs-number">100.00</span>%
Parser transition options: <span class="hljs-attr">system=projective,</span> <span class="hljs-attr">oracle=dynamic,</span> <span class="hljs-attr">structured_interval=10,</span> <span class="hljs-attr">single_root=1</span>
Parser uses lemmas/upos/xpos/feats: automatically generated by tagger
Parser embeddings options: <span class="hljs-attr">upostag=20,</span> <span class="hljs-attr">feats=20,</span> <span class="hljs-attr">xpostag=0,</span> <span class="hljs-attr">form=50,</span> <span class="hljs-attr">lemma=0,</span> <span class="hljs-attr">deprel=20</span>
  form <span class="hljs-attr">mincount=2,</span> precomputed form <span class="hljs-attr">embeddings=none</span>
  lemma <span class="hljs-attr">mincount=2,</span> precomputed lemma <span class="hljs-attr">embeddings=none</span>
Parser network options: <span class="hljs-attr">iterations=1,</span> <span class="hljs-attr">hidden_layer=200,</span> <span class="hljs-attr">batch_size=10,</span>
  <span class="hljs-attr">learning_rate=0.0100,</span> <span class="hljs-attr">learning_rate_final=0.0010,</span> <span class="hljs-attr">l2=0.5000,</span> <span class="hljs-attr">early_stopping=0</span>
Initialized 'universal_tag' embedding <span class="hljs-keyword">with</span> <span class="hljs-number">0</span>,<span class="hljs-number">9</span> words <span class="hljs-literal">and</span> <span class="hljs-number">0.0</span>%,<span class="hljs-number">100.0</span>% coverage.
Initialized 'feats' embedding <span class="hljs-keyword">with</span> <span class="hljs-number">0</span>,<span class="hljs-number">17</span> words <span class="hljs-literal">and</span> <span class="hljs-number">0.0</span>%,<span class="hljs-number">100.0</span>% coverage.
Initialized 'form' embedding <span class="hljs-keyword">with</span> <span class="hljs-number">0</span>,<span class="hljs-number">4</span> words <span class="hljs-literal">and</span> <span class="hljs-number">0.0</span>%,<span class="hljs-number">29.6</span>% coverage.
Initialized 'deprel' embedding <span class="hljs-keyword">with</span> <span class="hljs-number">0</span>,<span class="hljs-number">16</span> words <span class="hljs-literal">and</span> <span class="hljs-number">0.0</span>%,<span class="hljs-number">100.0</span>% coverage.
Iteration <span class="hljs-number">1</span>: training logprob -<span class="hljs-number">1.8848</span>e+<span class="hljs-number">002</span>
</code></pre>
<p>As you have seen above in the example, if you want to train the dependency parser, you can also provide pre-trained word embeddings which you can provide in the <code>embedding_form_file</code> argument. Example training data can be found at <a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-2364</a>. If you also have a holdout file in CONLL-U format which you can provide in the <code>files_conllu_holdout</code> argument, the training is stopped before model performance decreases on the holdout CONLL-U file.</p>
<p>Mark. Before you embark in starting to train your own models with more realistic learning parameters, consider that training can take a while.</p>
<h2><a class="anchor" aria-hidden="true" name="settings-for-the-tokenizer"></a><a href="#settings-for-the-tokenizer" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Settings for the tokenizer:</h2>
<p>The tokenizer recognizes the following options:</p>
<ul>
<li><code>tokenize_url</code> (default 1): tokenize URLs and emails using a manually implemented recognizer</li>
<li><code>allow_spaces</code> (default 1 if any token contains a space, 0 otherwise): allow tokens to contain spaces</li>
<li><code>dimension</code> (default 24): dimension of character embeddings and of the per-character bidirectional GRU. Note that inference time is quadratic in this parameter. Supported values are only 16, 24 and 64, with 64 needed only for languages with complicated tokenization like Japanese, Chinese or Vietnamese.</li>
<li><code>epochs</code> (default 100): the number of epochs to train the tokenizer for</li>
<li><code>batch_size</code> (default 50): batch size used during tokenizer training</li>
<li><code>learning_rate</code> (default 0.005): the learning rate used during tokenizer training</li>
<li><code>dropout</code> (default 0.1): dropout used during tokenizer training</li>
<li><code>early_stopping</code> (default 1 if heldout is given, 0 otherwise): perform early stopping, choosing training iteration maximizing sentences F1 score plus tokens F1 score on heldout data</li>
</ul>
<p>During random hyperparameter search, <code>batch_size</code> is chosen uniformly from {50,100} and <code>learning_rate</code> logarithmically from &lt;0.0005, 0.01).</p>
<p>The tokenizer is trained using the SpaceAfter=No features in the CoNLL-U files. If the feature is not present, a detokenizer can be used to guess the SpaceAfter=No features according to a supplied plain text (which typically does not overlap with the texts in the CoNLL-U files).</p>
<p>In order to use the detokenizer, use the <code>detokenizer=file:filename_with_plaintext</code> option. In UD 1.2 models, the optimal performance is achieved with very small plain texts â€“ only 500kB.</p>
<p>In order to show the settings which were used by the UDPipe community when building the models made available when using <code>udpipe_download_model</code>, the tokenizer settings used for the different treebanks are shown below, so that you can easily use this to retrain your model directly on the corresponding UD treebank which you can download at <a href="http://universaldependencies.org/#ud-treebanks">http://universaldependencies.org/#ud-treebanks</a>.</p>
<pre><code class="hljs css r">data(udpipe_annotation_params)
str(udpipe_annotation_params$tokenizer)
</code></pre>
<pre><code class="hljs">'data.frame':   <span class="hljs-number">68</span> obs. of  <span class="hljs-number">9</span> variables:
 $ language_treebank   : chr  "ar" "be" "bg" "ca" ...
 $ dimension           : num  <span class="hljs-number">24 24 64 64</span> <span class="hljs-number">24 64 24 24</span> <span class="hljs-number">64</span> <span class="hljs-number">24</span> ...
 $ epochs              : num  <span class="hljs-number">100 100 100 100</span> <span class="hljs-number">100 100 100 100</span> <span class="hljs-number">100 100</span> ...
 $ initialization_range: num  <span class="hljs-number">0.1 0.1</span> <span class="hljs-number">0.1 0.1</span> <span class="hljs-number">0.1 0.1</span> <span class="hljs-number">0.1 0.1</span> <span class="hljs-number">0.1 0.1</span> ...
 $ batch_size          : num  <span class="hljs-number">100 100 100 50</span> <span class="hljs-number">50 50 50 50</span> <span class="hljs-number">50</span> <span class="hljs-number">50</span> ...
 $ learning_rate       : num  <span class="hljs-number">0.002 0</span>.<span class="hljs-number">01 0.005</span> <span class="hljs-number">0.002 0</span>.<span class="hljs-number">01 0.002</span> <span class="hljs-number">0.005 0</span>.<span class="hljs-number">002</span> <span class="hljs-number">0.002 0</span>.<span class="hljs-number">01</span> ...
 $ dropout             : num  <span class="hljs-number">0.3 0.2</span> <span class="hljs-number">0.2 0.1</span> <span class="hljs-number">0.2 0.1</span> <span class="hljs-number">0.1 0.1</span> <span class="hljs-number">0.3 0.2</span> ...
 $ early_stopping      : num  <span class="hljs-number">1 1 1 1</span> <span class="hljs-number">1 1 1 1</span> <span class="hljs-number">1</span> <span class="hljs-number">1</span> ...
 $ detokenize          : chr  NA NA NA NA ...
</code></pre>
<pre><code class="hljs css r"><span class="hljs-comment">## Example for training the tokenizer on the Dutch treebank</span>
hyperparams_nl &lt;- subset(udpipe_annotation_params$tokenizer, language_treebank == <span class="hljs-string">"nl"</span>)
as.list(hyperparams_nl)
</code></pre>
<pre><code class="hljs"><span class="hljs-meta"><span class="hljs-meta-keyword">$language</span>_treebank</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"nl"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$dimension</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-number">24</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$epochs</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-number">100</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$initialization</span>_range</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$batch</span>_size</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">100</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$learning</span>_rate</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.005</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$dropout</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$early</span>_stopping</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$detokenize</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-literal">NA</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" name="settings-for-the-tagger"></a><a href="#settings-for-the-tagger" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Settings for the tagger:</h2>
<p>The tagging is currently performed using MorphoDiTa (<a href="http://ufal.mff.cuni.cz/morphodita">http://ufal.mff.cuni.cz/morphodita</a>). The UDPipe tagger consists of possibly several MorphoDiTa models, each tagging some of the POS tags and/or lemmas.</p>
<p>By default, only one model is constructed, which generates all available tags (UPOS, XPOS, Feats and Lemma). However, we found out during the UD 1.2 models training that performance improves if one model tags the UPOS, XPOS and Feats tags, while the other is performing lemmatization. Therefore, if you utilize two MorphoDiTa models, by default the first one generates all tags (except lemmas) and the second one performs lemmatization.</p>
<p>The number of MorphoDiTa models can be specified using the models=number parameter. All other parameters may be either generic for all models (guesser_suffix_rules=5), or specific for a given model (guesser_suffix_rules_2=6), including the from_model option (therefore, MorphoDiTa models can be trained separately and then combined together into one UDPipe model).</p>
<p>Every model utilizes UPOS for disambiguation and the first model is the one producing the UPOS tags on output.</p>
<p>The tagger recognizes the following options:</p>
<ul>
<li><code>use_lemma</code> (default for the second model and also if there is only one model): use the lemma field internally to perform disambiguation; the lemma may be not outputted</li>
<li><code>provide_lemma</code> (default for the second model and also if there is only one model): produce the disambiguated lemma on output</li>
<li><code>use_xpostag</code> (default for the first model): use the XPOS tags internally to perform disambiguation; it may not be outputted</li>
<li><code>provide_xpostag</code> (default for the first model): produce the disambiguated XPOS tag on output</li>
<li><code>use_feats</code> (default for the first model): use the Feats internally to perform disambiguation; it may not be outputted</li>
<li><code>provide_feats</code> (default for the first model): produce the disambiguated Feats field on output</li>
<li><code>dictionary_max_form_analyses</code> (default 0 - unlimited): the maximum number of (most frequent) form analyses from UD training data that are to be kept in the morphological dictionary</li>
<li><code>dictionary_file</code> (default empty): use a given custom morphological dictionary, where each line contains 5 tab-separated fields FORM, LEMMA, UPOSTAG, XPOSTAG and FEATS. Note that this dictionary data is appended to the dictionary created from the UD training data, not replacing it.</li>
<li><code>guesser_suffix_rules</code> (default 8): number of rules generated for every suffix</li>
<li><code>guesser_prefixes_max</code> (default 4 if provide_lemma, 0 otherwise): maximum number of form-generating prefixes to use in the guesser</li>
<li><code>guesser_prefix_min_count</code> (default 10): minimum number of occurrences of form-generating prefix to consider using it in the guesser</li>
<li><code>guesser_enrich_dictionary</code> (default 6 if no dictionary_file is passed, 0 otherwise): number of rules generated for forms present in training data (assuming that the analyses from the training data may not be all)</li>
<li><code>iterations</code> (default 20): number of training iterations to perform</li>
<li><code>early_stopping</code> (default 1 if heldout is given, 0 otherwise): perform early stopping, choosing training iteration maximizing tagging accuracy on the heldout data</li>
<li><code>templates</code> (default lemmatizer for second model, tagger otherwise): MorphoDiTa feature templates to use, either lemmatizer which focuses more on lemmas, or tagger which focuses more on UPOS/XPOS/FEATS</li>
</ul>
<p>During random hyperparameter search, guesser_suffix_rules is chosen uniformly from {5,6,7,8,9,10,11,12} and guesser_enrich_dictionary is chosen uniformly from {3,4,5,6,7,8,9,10}.</p>
<p>In order to show the settings which were used by the UDPipe community when building the models made available when using <code>udpipe_download_model</code>, the tagger settings used for the different treebanks are shown below, so that you can easily use this to retrain your model directly on the corresponding UD treebank which you can download at <a href="http://universaldependencies.org/#ud-treebanks">http://universaldependencies.org/#ud-treebanks</a>.</p>
<pre><code class="hljs css r"><span class="hljs-comment">## Example for training the tagger on the Dutch treebank</span>
hyperparams_nl &lt;- subset(udpipe_annotation_params$tagger, language_treebank == <span class="hljs-string">"nl"</span>)
as.list(hyperparams_nl)
</code></pre>
<pre><code class="hljs"><span class="hljs-meta"><span class="hljs-meta-keyword">$language</span>_treebank</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"nl"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$models</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-number">2</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$templates</span>_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"tagger"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$guesser</span>_suffix_rules_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">8</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$guesser</span>_enrich_dictionary_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">6</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$guesser</span>_prefixes_max_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$use</span>_lemma_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$use</span>_xpostag_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$use</span>_feats_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$provide</span>_lemma_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$provide</span>_xpostag_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$provide</span>_feats_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$prune</span>_features_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$templates</span>_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"lemmatizer"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$guesser</span>_suffix_rules_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">6</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$guesser</span>_enrich_dictionary_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">4</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$guesser</span>_prefixes_max_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">4</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$use</span>_lemma_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$use</span>_xpostag_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$use</span>_feats_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$provide</span>_lemma_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">1</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$provide</span>_xpostag_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$provide</span>_feats_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$prune</span>_features_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$dictionary</span>_max_form_analyses_2</span>
[<span class="hljs-number">1</span>] <span class="hljs-literal">NA</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$dictionary</span>_max_form_analyses_1</span>
[<span class="hljs-number">1</span>] <span class="hljs-literal">NA</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" name="settings-for-the-dependency-parser"></a><a href="#settings-for-the-dependency-parser" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Settings for the dependency parser:</h2>
<p>The parsing is performed using Parsito (<a href="http://ufal.mff.cuni.cz/parsito">http://ufal.mff.cuni.cz/parsito</a>), which is a transition-based parser using a neural-network classifier.</p>
<p>The transition-based systems can be configured by the following options:</p>
<ul>
<li><code>transition_system</code> (default projective): which transition system to use for parsing (language dependent, you can choose according to language properties or try all and choose the best one)
<code>projective</code>: projective stack-based arc standard system with shift, left_arc and right_arc transitions
<code>swap</code>: fully non-projective system which extends projective system by adding the swap transition
<code>link2</code>: partially non-projective system which extends projective system by adding left_arc2 and right_arc2 transitions</li>
<li><code>transition_oracle</code> (default dynamic/static_lazy_static whichever first is applicable): which transition oracle to use for the chosen transition_system:
<code>transition_system=projective</code>: available oracles are static and dynamic (dynamic usually gives better results, but training time is slower)
<code>transition_system=swap</code>: available oracles are static_eager and static_lazy (static_lazy almost always gives better results)
<code>transition_system=link2</code>: only available oracle is static</li>
<li><code>structured_interval</code> (default 8): use search-based oracle in addition to the translation_oracle specified. This almost always gives better results, but makes training 2-3 times slower. For details, see the paper Straka et al. 2015: Parsing Universal Dependency Treebanks using Neural Networks and Search-Based Oracle</li>
<li><code>single_root</code> (default 1): allow only single root when parsing, and make sure only the root node has the root deprel (note that training data are checked to be in this format)</li>
</ul>
<p>The Lemmas/UPOS/XPOS/FEATS used by the parser are configured by:</p>
<ul>
<li><code>use_gold_tags</code> (default 0): if false and a tagger exists, the Lemmas/UPOS/XPOS/FEATS for both the training and heldout data are generated by the tagger, otherwise they are taken from the gold data</li>
</ul>
<p>The embeddings used by the parser can be specified as follows:</p>
<ul>
<li><code>embedding_upostag</code> (default 20): the dimension of the UPos embedding used in the parser</li>
<li><code>embedding_feats</code> (default 20): the dimension of the Feats embedding used in the parser</li>
<li><code>embedding_xpostag</code> (default 0): the dimension of the XPos embedding used in the parser</li>
<li><code>embedding_form</code> (default 50): the dimension of the Form embedding used in the parser</li>
<li><code>embedding_lemma</code> (default 0): the dimension of the Lemma embedding used in the parser</li>
<li><code>embedding_deprel</code> (default 20): the dimension of the Deprel embedding used in the parser</li>
<li><code>embedding_form_file</code>: pre-trained word embeddings in word2vec textual format</li>
<li><code>embedding_lemma_file</code>: pre-trained lemma embeddings in word2vec textual format</li>
<li><code>embedding_form_mincount</code> (default 2): for forms not present in the pre-trained embeddings, generate random embeddings if the form appears at least this number of times in the trainig data (forms not present in the pre-trained embeddings and appearing less number of times are considered OOV)</li>
<li><code>embedding_lemma_mincount</code> (default 2): for lemmas not present in the pre-trained embeddings, generate random embeddings if the lemma appears at least this number of times in the trainig data (lemmas not present in the pre-trained embeddings and appearing less number of times are considered OOV)</li>
</ul>
<p>The neural-network training options:</p>
<ul>
<li><code>iterations</code> (default 10): number of training iterations to use</li>
<li><code>hidden_layer</code> (default 200): the size of the hidden layer</li>
<li><code>batch_size</code> (default 10): batch size used during neural-network training</li>
<li><code>learning_rate</code> (default 0.02): the learning rate used during neural-network training</li>
<li><code>learning_rate_final</code> (0.001): the final learning rate used during neural-network training</li>
<li><code>l2</code> (0.5): the L2 regularization used during neural-network training</li>
<li><code>early_stopping</code> (default 1 if heldout is given, 0 otherwise): perform early stopping, choosing training iteration maximizing LAS on heldout data</li>
</ul>
<p>During random hyperparameter search, structured_interval is chosen uniformly from {0,8,10}, learning_rate is chosen logarithmically from &lt;0.005,0.04) and l2 is chosen uniformly from &lt;0.2,0.6).</p>
<p>In order to show the settings which were used by the UDPipe community when building the models made available when using <code>udpipe_download_model</code>, the parser settings used for the different treebanks are shown below, so that you can easily use this to retrain your model directly on the corresponding UD treebank which you can download at <a href="http://universaldependencies.org/#ud-treebanks">http://universaldependencies.org/#ud-treebanks</a>.</p>
<pre><code class="hljs css r"><span class="hljs-comment">## Example for training the dependency parser on the Dutch treebank</span>
hyperparams_nl &lt;- subset(udpipe_annotation_params$parser, language_treebank == <span class="hljs-string">"nl"</span>)
as.list(hyperparams_nl)
</code></pre>
<pre><code class="hljs"><span class="hljs-meta"><span class="hljs-meta-keyword">$language</span>_treebank</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"nl"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$iterations</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-number">30</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_upostag</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">20</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_feats</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">20</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_xpostag</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_form</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">50</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_form_file</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"../ud-2.0-embeddings/nl.skip.forms.50.vectors"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_lemma</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$embedding</span>_deprel</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">20</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$learning</span>_rate</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.01</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$learning</span>_rate_final</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.001</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$l2</span></span>
[<span class="hljs-number">1</span>] <span class="hljs-number">0.5</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$hidden</span>_layer</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">200</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$batch</span>_size</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">10</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$transition</span>_system</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"projective"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$transition</span>_oracle</span>
[<span class="hljs-number">1</span>] <span class="hljs-string">"dynamic"</span>

<span class="hljs-meta"><span class="hljs-meta-keyword">$structured</span>_interval</span>
[<span class="hljs-number">1</span>] <span class="hljs-number">10</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" name="example"></a><a href="#example" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example</h2>
<h3><a class="anchor" aria-hidden="true" name="example-on-ud-26-on-german-gsd"></a><a href="#example-on-ud-26-on-german-gsd" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Example on UD 2.6 on German GSD</h3>
<p>The following code shows a complete training run on the German GSD treebank from UD 2.6. The code</p>
<ol>
<li>downloads the Universal Dependencies training data</li>
<li>builds a word2vec model on top of the training data as this is used in the dependency parser</li>
<li>trains the model using the set of hyperparameters which were used by the UDPipe authors and which you can find at <a href="https://github.com/ufal/udpipe/tree/master/training">https://github.com/ufal/udpipe/tree/master/training</a> in general or at <a href="https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131">https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-3131</a> for UD 2.5</li>
</ol>
<pre><code class="hljs css r"><span class="hljs-keyword">library</span>(utils)
<span class="hljs-keyword">library</span>(udpipe)
<span class="hljs-keyword">library</span>(word2vec)

<span class="hljs-comment">## Work on data from Universal Dependencies - German GSD treebank</span>
settings &lt;- list()
settings$ud.train    &lt;- <span class="hljs-string">"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/r2.6/de_gsd-ud-train.conllu"</span>
settings$ud.dev      &lt;- <span class="hljs-string">"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/r2.6/de_gsd-ud-dev.conllu"</span>
settings$ud.test     &lt;- <span class="hljs-string">"https://raw.githubusercontent.com/UniversalDependencies/UD_German-GSD/r2.6/de_gsd-ud-test.conllu"</span>

<span class="hljs-comment">## Download the conllu files</span>
download.file(url = settings$ud.train, destfile = <span class="hljs-string">"train.conllu"</span>)
download.file(url = settings$ud.dev,   destfile = <span class="hljs-string">"dev.conllu"</span>)
download.file(url = settings$ud.test,  destfile = <span class="hljs-string">"test.conllu"</span>)

<span class="hljs-comment">## Create wordvectors as these are used for training the dependency parser + save the word vectors to disk</span>
x &lt;- udpipe_read_conllu(<span class="hljs-string">"train.conllu"</span>)
x &lt;- paste.data.frame(x, term = <span class="hljs-string">"token"</span>, group = c(<span class="hljs-string">"doc_id"</span>, <span class="hljs-string">"paragraph_id"</span>, <span class="hljs-string">"sentence_id"</span>), collapse = <span class="hljs-string">" "</span>)
x &lt;- x$token
writeLines(x, con = file(<span class="hljs-string">"text.txt"</span>, encoding = <span class="hljs-string">"UTF-8"</span>, open = <span class="hljs-string">"wt"</span>))
w2v &lt;- word2vec(<span class="hljs-string">"text.txt"</span>, type = <span class="hljs-string">"skip-gram"</span>, dim = <span class="hljs-number">50</span>, window = <span class="hljs-number">10</span>, min_count = <span class="hljs-number">2</span>, negative = <span class="hljs-number">5</span>, iter = <span class="hljs-number">15</span>, threads = <span class="hljs-number">1</span>)
write.word2vec(w2v, file = <span class="hljs-string">"wordvectors.vec"</span>, type = <span class="hljs-string">"txt"</span>, encoding = <span class="hljs-string">"UTF-8"</span>)
predict(w2v, c(<span class="hljs-string">"gut"</span>, <span class="hljs-string">"freundlich"</span>), type = <span class="hljs-string">"nearest"</span>, top = <span class="hljs-number">20</span>)

<span class="hljs-comment">## Train the model</span>
print(Sys.time())
m &lt;- udpipe_train(file = <span class="hljs-string">"de_gsd-ud-2.6-20200924.udpipe"</span>, 
                  files_conllu_training = <span class="hljs-string">"train.conllu"</span>, 
                  files_conllu_holdout  = <span class="hljs-string">"dev.conllu"</span>,
                  annotation_tokenizer = list(dimension = <span class="hljs-number">64</span>, epochs = <span class="hljs-number">100</span>, segment_size=<span class="hljs-number">200</span>, initialization_range = <span class="hljs-number">0.1</span>, 
                                              batch_size = <span class="hljs-number">50</span>, learning_rate = <span class="hljs-number">0.002</span>, learning_rate_final=<span class="hljs-number">0</span>, dropout = <span class="hljs-number">0.1</span>, early_stopping = <span class="hljs-number">1</span>),
                  annotation_tagger = list(models = <span class="hljs-number">2</span>, 
                                           templates_1 = <span class="hljs-string">"lemmatizer"</span>, guesser_suffix_rules_1 = <span class="hljs-number">8</span>, guesser_enrich_dictionary_1 = <span class="hljs-number">4</span>, guesser_prefixes_max_1 = <span class="hljs-number">4</span>, 
                                           use_lemma_1 = <span class="hljs-number">1</span>,provide_lemma_1 = <span class="hljs-number">1</span>, use_xpostag_1 = <span class="hljs-number">0</span>, provide_xpostag_1 = <span class="hljs-number">0</span>, 
                                           use_feats_1 = <span class="hljs-number">0</span>, provide_feats_1 = <span class="hljs-number">0</span>, prune_features_1 = <span class="hljs-number">1</span>, 
                                           templates_2 = <span class="hljs-string">"tagger"</span>, guesser_suffix_rules_2 = <span class="hljs-number">8</span>, guesser_enrich_dictionary_2 = <span class="hljs-number">4</span>, guesser_prefixes_max_2 = <span class="hljs-number">0</span>, 
                                           use_lemma_2 = <span class="hljs-number">1</span>, provide_lemma_2 = <span class="hljs-number">0</span>, use_xpostag_2 = <span class="hljs-number">1</span>, provide_xpostag_2 = <span class="hljs-number">1</span>, 
                                           use_feats_2 = <span class="hljs-number">1</span>, provide_feats_2 = <span class="hljs-number">1</span>, prune_features_2 = <span class="hljs-number">1</span>),
                  annotation_parser = list(iterations = <span class="hljs-number">30</span>, embedding_upostag = <span class="hljs-number">20</span>, embedding_feats = <span class="hljs-number">20</span>, embedding_xpostag = <span class="hljs-number">0</span>, 
                                           embedding_form = <span class="hljs-number">50</span>, embedding_form_file = <span class="hljs-string">"wordvectors.vec"</span>, 
                                           embedding_lemma = <span class="hljs-number">0</span>, embedding_deprel = <span class="hljs-number">20</span>, learning_rate = <span class="hljs-number">0.01</span>, 
                                           learning_rate_final = <span class="hljs-number">0.001</span>, l2 = <span class="hljs-number">0.5</span>, hidden_layer = <span class="hljs-number">200</span>, 
                                           batch_size = <span class="hljs-number">10</span>, transition_system = <span class="hljs-string">"projective"</span>, transition_oracle = <span class="hljs-string">"dynamic"</span>, 
                                           structured_interval = <span class="hljs-number">8</span>))
print(Sys.time())

<span class="hljs-comment">## Evaluate the accuracy</span>
m &lt;- udpipe_load_model(<span class="hljs-string">"de_gsd-ud-2.6-20200924.udpipe"</span>)
goodness_of_fit &lt;- udpipe_accuracy(m, <span class="hljs-string">"test.conllu"</span>, tokenizer = <span class="hljs-string">"default"</span>, tagger = <span class="hljs-string">"default"</span>, parser = <span class="hljs-string">"default"</span>)
cat(goodness_of_fit$accuracy, sep = <span class="hljs-string">"\n"</span>) 
goodness_of_fit &lt;- udpipe_accuracy(m, <span class="hljs-string">"test.conllu"</span>, tokenizer = <span class="hljs-string">"none"</span>, tagger = <span class="hljs-string">"default"</span>, parser = <span class="hljs-string">"default"</span>)
cat(goodness_of_fit$accuracy, sep = <span class="hljs-string">"\n"</span>) 
goodness_of_fit &lt;- udpipe_accuracy(m, <span class="hljs-string">"test.conllu"</span>, tokenizer = <span class="hljs-string">"none"</span>, tagger = <span class="hljs-string">"none"</span>, parser = <span class="hljs-string">"default"</span>)
cat(goodness_of_fit$accuracy, sep = <span class="hljs-string">"\n"</span>) 
</code></pre>
<p>This will give you something similar to this:</p>
<pre><code class="hljs">&gt; <span class="hljs-keyword">print</span>(Sys.time())
[<span class="hljs-number">1</span>] <span class="hljs-string">"2020-09-24 22:42:30 CEST"</span>
&gt; m &lt;- udpipe_train(<span class="hljs-keyword">file</span> = <span class="hljs-string">"de_gsd-ud-2.6-20200924.udpipe"</span>, 
+                   files_conllu_training = <span class="hljs-string">"train.conllu"</span>, 
+                   files_conllu_holdout  = <span class="hljs-string">"dev.conllu"</span>,
+                   annotation_tokenizer = list(dimension = <span class="hljs-number">64</span>, epochs = <span class="hljs-number">100</span>, segment_size=<span class="hljs-number">200</span>, initialization_range = <span class="hljs-number">0.1</span>, 
+                                               batch_size = <span class="hljs-number">50</span>, learning_rate = <span class="hljs-number">0.002</span>, learning_rate_final=<span class="hljs-number">0</span>, dropout = <span class="hljs-number">0.1</span>, early_stopping = <span class="hljs-number">1</span>),
+                   annotation_tagger = list(models = <span class="hljs-number">2</span>, 
+                                            templates_1 = <span class="hljs-string">"lemmatizer"</span>, guesser_suffix_rules_1 = <span class="hljs-number">8</span>, guesser_enrich_dictionary_1 = <span class="hljs-number">4</span>, guesser_prefixes_max_1 = <span class="hljs-number">4</span>, 
+                                            use_lemma_1 = <span class="hljs-number">1</span>,provide_lemma_1 = <span class="hljs-number">1</span>, use_xpostag_1 = <span class="hljs-number">0</span>, provide_xpostag_1 = <span class="hljs-number">0</span>, 
+                                            use_feats_1 = <span class="hljs-number">0</span>, provide_feats_1 = <span class="hljs-number">0</span>, prune_features_1 = <span class="hljs-number">1</span>, 
+                                            templates_2 = <span class="hljs-string">"tagger"</span>, guesser_suffix_rules_2 = <span class="hljs-number">8</span>, guesser_enrich_dictionary_2 = <span class="hljs-number">4</span>, guesser_prefixes_max_2 = <span class="hljs-number">0</span>, 
+                                            use_lemma_2 = <span class="hljs-number">1</span>, provide_lemma_2 = <span class="hljs-number">0</span>, use_xpostag_2 = <span class="hljs-number">1</span>, provide_xpostag_2 = <span class="hljs-number">1</span>, 
+                                            use_feats_2 = <span class="hljs-number">1</span>, provide_feats_2 = <span class="hljs-number">1</span>, prune_features_2 = <span class="hljs-number">1</span>),
+                   annotation_parser = list(iterations = <span class="hljs-number">30</span>, embedding_upostag = <span class="hljs-number">20</span>, embedding_feats = <span class="hljs-number">20</span>, embedding_xpostag = <span class="hljs-number">0</span>, 
+                                            embedding_form = <span class="hljs-number">50</span>, embedding_form_file = <span class="hljs-string">"wordvectors.vec"</span>, 
+                                            embedding_lemma = <span class="hljs-number">0</span>, embedding_deprel = <span class="hljs-number">20</span>, learning_rate = <span class="hljs-number">0.01</span>, 
+                                            learning_rate_final = <span class="hljs-number">0.001</span>, l2 = <span class="hljs-number">0.5</span>, hidden_layer = <span class="hljs-number">200</span>, 
+                                            batch_size = <span class="hljs-number">10</span>, transition_system = <span class="hljs-string">"projective"</span>, transition_oracle = <span class="hljs-string">"dynamic"</span>, 
+                                            structured_interval = <span class="hljs-number">8</span>))
Training tokenizer with the following options: tokenize_url=<span class="hljs-number">1</span>, allow_spaces=<span class="hljs-number">0</span>, dimension=<span class="hljs-number">64</span>
  epochs=<span class="hljs-number">100</span>, batch_size=<span class="hljs-number">50</span>, learning_rate=<span class="hljs-number">0.0020</span>, dropout=<span class="hljs-number">0.1000</span>, early_stopping=<span class="hljs-number">1</span>
Epoch <span class="hljs-number">1</span>, logprob: <span class="hljs-number">-6.9193e+004</span>, training acc: <span class="hljs-number">95.43</span>%, heldout tokens: <span class="hljs-number">97.44</span>%P/<span class="hljs-number">96.73</span>%R/<span class="hljs-number">97.08</span>%, sentences: <span class="hljs-number">79.03</span>%P/<span class="hljs-number">79.22</span>%R/<span class="hljs-number">79.12</span>%
Epoch <span class="hljs-number">2</span>, logprob: <span class="hljs-number">-5.3557e+003</span>, training acc: <span class="hljs-number">99.58</span>%, heldout tokens: <span class="hljs-number">98.33</span>%P/<span class="hljs-number">98.73</span>%R/<span class="hljs-number">98.53</span>%, sentences: <span class="hljs-number">82.25</span>%P/<span class="hljs-number">84.11</span>%R/<span class="hljs-number">83.17</span>%
Epoch <span class="hljs-number">3</span>, logprob: <span class="hljs-number">-4.7095e+003</span>, training acc: <span class="hljs-number">99.62</span>%, heldout tokens: <span class="hljs-number">98.84</span>%P/<span class="hljs-number">99.25</span>%R/<span class="hljs-number">99.04</span>%, sentences: <span class="hljs-number">82.72</span>%P/<span class="hljs-number">87.48</span>%R/<span class="hljs-number">85.04</span>%
Epoch <span class="hljs-number">4</span>, logprob: <span class="hljs-number">-4.2784e+003</span>, training acc: <span class="hljs-number">99.65</span>%, heldout tokens: <span class="hljs-number">98.99</span>%P/<span class="hljs-number">99.33</span>%R/<span class="hljs-number">99.16</span>%, sentences: <span class="hljs-number">86.53</span>%P/<span class="hljs-number">89.24</span>%R/<span class="hljs-number">87.86</span>%
Epoch <span class="hljs-number">5</span>, logprob: <span class="hljs-number">-3.9963e+003</span>, training acc: <span class="hljs-number">99.67</span>%, heldout tokens: <span class="hljs-number">99.37</span>%P/<span class="hljs-number">99.43</span>%R/<span class="hljs-number">99.40</span>%, sentences: <span class="hljs-number">88.16</span>%P/<span class="hljs-number">89.49</span>%R/<span class="hljs-number">88.82</span>%
Epoch <span class="hljs-number">6</span>, logprob: <span class="hljs-number">-3.6930e+003</span>, training acc: <span class="hljs-number">99.70</span>%, heldout tokens: <span class="hljs-number">99.47</span>%P/<span class="hljs-number">99.29</span>%R/<span class="hljs-number">99.38</span>%, sentences: <span class="hljs-number">91.18</span>%P/<span class="hljs-number">90.61</span>%R/<span class="hljs-number">90.90</span>%
Epoch <span class="hljs-number">7</span>, logprob: <span class="hljs-number">-3.6024e+003</span>, training acc: <span class="hljs-number">99.72</span>%, heldout tokens: <span class="hljs-number">99.56</span>%P/<span class="hljs-number">99.59</span>%R/<span class="hljs-number">99.58</span>%, sentences: <span class="hljs-number">90.19</span>%P/<span class="hljs-number">90.86</span>%R/<span class="hljs-number">90.52</span>%
Epoch <span class="hljs-number">8</span>, logprob: <span class="hljs-number">-3.3887e+003</span>, training acc: <span class="hljs-number">99.74</span>%, heldout tokens: <span class="hljs-number">99.26</span>%P/<span class="hljs-number">99.46</span>%R/<span class="hljs-number">99.36</span>%, sentences: <span class="hljs-number">89.80</span>%P/<span class="hljs-number">91.49</span>%R/<span class="hljs-number">90.64</span>%
Epoch <span class="hljs-number">9</span>, logprob: <span class="hljs-number">-3.3730e+003</span>, training acc: <span class="hljs-number">99.74</span>%, heldout tokens: <span class="hljs-number">99.61</span>%P/<span class="hljs-number">99.72</span>%R/<span class="hljs-number">99.66</span>%, sentences: <span class="hljs-number">91.81</span>%P/<span class="hljs-number">91.24</span>%R/<span class="hljs-number">91.53</span>%
Epoch <span class="hljs-number">10</span>, logprob: <span class="hljs-number">-3.2727e+003</span>, training acc: <span class="hljs-number">99.74</span>%, heldout tokens: <span class="hljs-number">99.52</span>%P/<span class="hljs-number">99.59</span>%R/<span class="hljs-number">99.55</span>%, sentences: <span class="hljs-number">89.15</span>%P/<span class="hljs-number">91.49</span>%R/<span class="hljs-number">90.30</span>%
Epoch <span class="hljs-number">11</span>, logprob: <span class="hljs-number">-3.2605e+003</span>, training acc: <span class="hljs-number">99.74</span>%, heldout tokens: <span class="hljs-number">99.68</span>%P/<span class="hljs-number">99.69</span>%R/<span class="hljs-number">99.68</span>%, sentences: <span class="hljs-number">91.60</span>%P/<span class="hljs-number">91.49</span>%R/<span class="hljs-number">91.55</span>%
Epoch <span class="hljs-number">12</span>, logprob: <span class="hljs-number">-3.1610e+003</span>, training acc: <span class="hljs-number">99.75</span>%, heldout tokens: <span class="hljs-number">99.66</span>%P/<span class="hljs-number">99.77</span>%R/<span class="hljs-number">99.72</span>%, sentences: <span class="hljs-number">89.52</span>%P/<span class="hljs-number">90.86</span>%R/<span class="hljs-number">90.19</span>%
Epoch <span class="hljs-number">13</span>, logprob: <span class="hljs-number">-3.1630e+003</span>, training acc: <span class="hljs-number">99.75</span>%, heldout tokens: <span class="hljs-number">99.58</span>%P/<span class="hljs-number">99.55</span>%R/<span class="hljs-number">99.56</span>%, sentences: <span class="hljs-number">92.69</span>%P/<span class="hljs-number">91.99</span>%R/<span class="hljs-number">92.34</span>%
Epoch <span class="hljs-number">14</span>, logprob: <span class="hljs-number">-3.0341e+003</span>, training acc: <span class="hljs-number">99.76</span>%, heldout tokens: <span class="hljs-number">99.59</span>%P/<span class="hljs-number">99.60</span>%R/<span class="hljs-number">99.60</span>%, sentences: <span class="hljs-number">91.59</span>%P/<span class="hljs-number">91.36</span>%R/<span class="hljs-number">91.48</span>%
Epoch <span class="hljs-number">15</span>, logprob: <span class="hljs-number">-2.9909e+003</span>, training acc: <span class="hljs-number">99.77</span>%, heldout tokens: <span class="hljs-number">99.57</span>%P/<span class="hljs-number">99.71</span>%R/<span class="hljs-number">99.64</span>%, sentences: <span class="hljs-number">91.79</span>%P/<span class="hljs-number">92.37</span>%R/<span class="hljs-number">92.08</span>%
Epoch <span class="hljs-number">16</span>, logprob: <span class="hljs-number">-2.9968e+003</span>, training acc: <span class="hljs-number">99.77</span>%, heldout tokens: <span class="hljs-number">99.70</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.73</span>%, sentences: <span class="hljs-number">91.30</span>%P/<span class="hljs-number">91.99</span>%R/<span class="hljs-number">91.65</span>%
Epoch <span class="hljs-number">17</span>, logprob: <span class="hljs-number">-2.8742e+003</span>, training acc: <span class="hljs-number">99.78</span>%, heldout tokens: <span class="hljs-number">99.62</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.69</span>%, sentences: <span class="hljs-number">91.65</span>%P/<span class="hljs-number">91.99</span>%R/<span class="hljs-number">91.82</span>%
Epoch <span class="hljs-number">18</span>, logprob: <span class="hljs-number">-2.8592e+003</span>, training acc: <span class="hljs-number">99.77</span>%, heldout tokens: <span class="hljs-number">99.71</span>%P/<span class="hljs-number">99.69</span>%R/<span class="hljs-number">99.70</span>%, sentences: <span class="hljs-number">92.10</span>%P/<span class="hljs-number">91.86</span>%R/<span class="hljs-number">91.98</span>%
Epoch <span class="hljs-number">19</span>, logprob: <span class="hljs-number">-2.8845e+003</span>, training acc: <span class="hljs-number">99.77</span>%, heldout tokens: <span class="hljs-number">99.62</span>%P/<span class="hljs-number">99.68</span>%R/<span class="hljs-number">99.65</span>%, sentences: <span class="hljs-number">92.42</span>%P/<span class="hljs-number">93.12</span>%R/<span class="hljs-number">92.77</span>%
Epoch <span class="hljs-number">20</span>, logprob: <span class="hljs-number">-2.8405e+003</span>, training acc: <span class="hljs-number">99.77</span>%, heldout tokens: <span class="hljs-number">99.68</span>%P/<span class="hljs-number">99.71</span>%R/<span class="hljs-number">99.69</span>%, sentences: <span class="hljs-number">93.20</span>%P/<span class="hljs-number">92.62</span>%R/<span class="hljs-number">92.91</span>%
Epoch <span class="hljs-number">21</span>, logprob: <span class="hljs-number">-2.7477e+003</span>, training acc: <span class="hljs-number">99.78</span>%, heldout tokens: <span class="hljs-number">99.69</span>%P/<span class="hljs-number">99.78</span>%R/<span class="hljs-number">99.74</span>%, sentences: <span class="hljs-number">89.19</span>%P/<span class="hljs-number">90.86</span>%R/<span class="hljs-number">90.02</span>%
Epoch <span class="hljs-number">22</span>, logprob: <span class="hljs-number">-2.7414e+003</span>, training acc: <span class="hljs-number">99.78</span>%, heldout tokens: <span class="hljs-number">99.59</span>%P/<span class="hljs-number">99.65</span>%R/<span class="hljs-number">99.62</span>%, sentences: <span class="hljs-number">90.73</span>%P/<span class="hljs-number">91.86</span>%R/<span class="hljs-number">91.29</span>%
Epoch <span class="hljs-number">23</span>, logprob: <span class="hljs-number">-2.6864e+003</span>, training acc: <span class="hljs-number">99.79</span>%, heldout tokens: <span class="hljs-number">99.66</span>%P/<span class="hljs-number">99.59</span>%R/<span class="hljs-number">99.63</span>%, sentences: <span class="hljs-number">92.80</span>%P/<span class="hljs-number">93.62</span>%R/<span class="hljs-number">93.21</span>%
Epoch <span class="hljs-number">24</span>, logprob: <span class="hljs-number">-2.4937e+003</span>, training acc: <span class="hljs-number">99.80</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.74</span>%, sentences: <span class="hljs-number">92.67</span>%P/<span class="hljs-number">93.37</span>%R/<span class="hljs-number">93.02</span>%
Epoch <span class="hljs-number">25</span>, logprob: <span class="hljs-number">-2.6481e+003</span>, training acc: <span class="hljs-number">99.79</span>%, heldout tokens: <span class="hljs-number">99.73</span>%P/<span class="hljs-number">99.78</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">91.96</span>%P/<span class="hljs-number">92.99</span>%R/<span class="hljs-number">92.47</span>%
Epoch <span class="hljs-number">26</span>, logprob: <span class="hljs-number">-2.6341e+003</span>, training acc: <span class="hljs-number">99.80</span>%, heldout tokens: <span class="hljs-number">99.66</span>%P/<span class="hljs-number">99.75</span>%R/<span class="hljs-number">99.70</span>%, sentences: <span class="hljs-number">93.12</span>%P/<span class="hljs-number">93.24</span>%R/<span class="hljs-number">93.18</span>%
Epoch <span class="hljs-number">27</span>, logprob: <span class="hljs-number">-2.6092e+003</span>, training acc: <span class="hljs-number">99.79</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.72</span>%R/<span class="hljs-number">99.72</span>%, sentences: <span class="hljs-number">93.37</span>%P/<span class="hljs-number">93.37</span>%R/<span class="hljs-number">93.37</span>%
Epoch <span class="hljs-number">28</span>, logprob: <span class="hljs-number">-2.5791e+003</span>, training acc: <span class="hljs-number">99.79</span>%, heldout tokens: <span class="hljs-number">99.66</span>%P/<span class="hljs-number">99.59</span>%R/<span class="hljs-number">99.63</span>%, sentences: <span class="hljs-number">94.51</span>%P/<span class="hljs-number">92.62</span>%R/<span class="hljs-number">93.55</span>%
Epoch <span class="hljs-number">29</span>, logprob: <span class="hljs-number">-2.4441e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.69</span>%P/<span class="hljs-number">99.69</span>%R/<span class="hljs-number">99.69</span>%, sentences: <span class="hljs-number">94.45</span>%P/<span class="hljs-number">93.74</span>%R/<span class="hljs-number">94.10</span>%
Epoch <span class="hljs-number">30</span>, logprob: <span class="hljs-number">-2.6083e+003</span>, training acc: <span class="hljs-number">99.80</span>%, heldout tokens: <span class="hljs-number">99.74</span>%P/<span class="hljs-number">99.77</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">94.01</span>%P/<span class="hljs-number">94.24</span>%R/<span class="hljs-number">94.12</span>%
Epoch <span class="hljs-number">31</span>, logprob: <span class="hljs-number">-2.5544e+003</span>, training acc: <span class="hljs-number">99.80</span>%, heldout tokens: <span class="hljs-number">99.47</span>%P/<span class="hljs-number">99.26</span>%R/<span class="hljs-number">99.37</span>%, sentences: <span class="hljs-number">92.49</span>%P/<span class="hljs-number">92.49</span>%R/<span class="hljs-number">92.49</span>%
Epoch <span class="hljs-number">32</span>, logprob: <span class="hljs-number">-2.4888e+003</span>, training acc: <span class="hljs-number">99.79</span>%, heldout tokens: <span class="hljs-number">99.76</span>%P/<span class="hljs-number">99.79</span>%R/<span class="hljs-number">99.78</span>%, sentences: <span class="hljs-number">94.20</span>%P/<span class="hljs-number">93.49</span>%R/<span class="hljs-number">93.84</span>%
Epoch <span class="hljs-number">33</span>, logprob: <span class="hljs-number">-2.5314e+003</span>, training acc: <span class="hljs-number">99.80</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.73</span>%R/<span class="hljs-number">99.72</span>%, sentences: <span class="hljs-number">93.25</span>%P/<span class="hljs-number">93.37</span>%R/<span class="hljs-number">93.31</span>%
Epoch <span class="hljs-number">34</span>, logprob: <span class="hljs-number">-2.4597e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.74</span>%P/<span class="hljs-number">99.80</span>%R/<span class="hljs-number">99.77</span>%, sentences: <span class="hljs-number">93.23</span>%P/<span class="hljs-number">93.12</span>%R/<span class="hljs-number">93.17</span>%
Epoch <span class="hljs-number">35</span>, logprob: <span class="hljs-number">-2.3949e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.68</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.72</span>%, sentences: <span class="hljs-number">91.81</span>%P/<span class="hljs-number">92.62</span>%R/<span class="hljs-number">92.21</span>%
Epoch <span class="hljs-number">36</span>, logprob: <span class="hljs-number">-2.3642e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.71</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.74</span>%, sentences: <span class="hljs-number">94.09</span>%P/<span class="hljs-number">93.62</span>%R/<span class="hljs-number">93.85</span>%
Epoch <span class="hljs-number">37</span>, logprob: <span class="hljs-number">-2.4537e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.73</span>%P/<span class="hljs-number">99.80</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">92.04</span>%P/<span class="hljs-number">92.62</span>%R/<span class="hljs-number">92.33</span>%
Epoch <span class="hljs-number">38</span>, logprob: <span class="hljs-number">-2.4156e+003</span>, training acc: <span class="hljs-number">99.80</span>%, heldout tokens: <span class="hljs-number">99.74</span>%P/<span class="hljs-number">99.79</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">93.75</span>%P/<span class="hljs-number">91.99</span>%R/<span class="hljs-number">92.86</span>%
Epoch <span class="hljs-number">39</span>, logprob: <span class="hljs-number">-2.4693e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.59</span>%P/<span class="hljs-number">99.42</span>%R/<span class="hljs-number">99.51</span>%, sentences: <span class="hljs-number">93.23</span>%P/<span class="hljs-number">93.12</span>%R/<span class="hljs-number">93.17</span>%
Epoch <span class="hljs-number">40</span>, logprob: <span class="hljs-number">-2.3308e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.67</span>%P/<span class="hljs-number">99.63</span>%R/<span class="hljs-number">99.65</span>%, sentences: <span class="hljs-number">93.14</span>%P/<span class="hljs-number">93.49</span>%R/<span class="hljs-number">93.32</span>%
Epoch <span class="hljs-number">41</span>, logprob: <span class="hljs-number">-2.3365e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.70</span>%P/<span class="hljs-number">99.73</span>%R/<span class="hljs-number">99.72</span>%, sentences: <span class="hljs-number">92.73</span>%P/<span class="hljs-number">92.62</span>%R/<span class="hljs-number">92.67</span>%
Epoch <span class="hljs-number">42</span>, logprob: <span class="hljs-number">-2.3616e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.75</span>%R/<span class="hljs-number">99.73</span>%, sentences: <span class="hljs-number">93.14</span>%P/<span class="hljs-number">93.49</span>%R/<span class="hljs-number">93.32</span>%
Epoch <span class="hljs-number">43</span>, logprob: <span class="hljs-number">-2.3260e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.78</span>%P/<span class="hljs-number">99.83</span>%R/<span class="hljs-number">99.81</span>%, sentences: <span class="hljs-number">93.49</span>%P/<span class="hljs-number">93.49</span>%R/<span class="hljs-number">93.49</span>%
Epoch <span class="hljs-number">44</span>, logprob: <span class="hljs-number">-2.3651e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.73</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.74</span>%, sentences: <span class="hljs-number">92.14</span>%P/<span class="hljs-number">92.49</span>%R/<span class="hljs-number">92.32</span>%
Epoch <span class="hljs-number">45</span>, logprob: <span class="hljs-number">-2.2028e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.69</span>%P/<span class="hljs-number">99.70</span>%R/<span class="hljs-number">99.70</span>%, sentences: <span class="hljs-number">94.21</span>%P/<span class="hljs-number">93.62</span>%R/<span class="hljs-number">93.91</span>%
Epoch <span class="hljs-number">46</span>, logprob: <span class="hljs-number">-2.2973e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.62</span>%P/<span class="hljs-number">99.60</span>%R/<span class="hljs-number">99.61</span>%, sentences: <span class="hljs-number">93.62</span>%P/<span class="hljs-number">93.74</span>%R/<span class="hljs-number">93.68</span>%
Epoch <span class="hljs-number">47</span>, logprob: <span class="hljs-number">-2.3190e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.77</span>%P/<span class="hljs-number">99.79</span>%R/<span class="hljs-number">99.78</span>%, sentences: <span class="hljs-number">93.52</span>%P/<span class="hljs-number">93.87</span>%R/<span class="hljs-number">93.69</span>%
Epoch <span class="hljs-number">48</span>, logprob: <span class="hljs-number">-2.2889e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.74</span>%P/<span class="hljs-number">99.75</span>%R/<span class="hljs-number">99.74</span>%, sentences: <span class="hljs-number">93.34</span>%P/<span class="hljs-number">92.99</span>%R/<span class="hljs-number">93.17</span>%
Epoch <span class="hljs-number">49</span>, logprob: <span class="hljs-number">-2.2343e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.75</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">93.63</span>%P/<span class="hljs-number">93.87</span>%R/<span class="hljs-number">93.75</span>%
Epoch <span class="hljs-number">50</span>, logprob: <span class="hljs-number">-2.1653e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.73</span>%R/<span class="hljs-number">99.72</span>%, sentences: <span class="hljs-number">93.85</span>%P/<span class="hljs-number">93.62</span>%R/<span class="hljs-number">93.73</span>%
Epoch <span class="hljs-number">51</span>, logprob: <span class="hljs-number">-2.1677e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.76</span>%P/<span class="hljs-number">99.82</span>%R/<span class="hljs-number">99.79</span>%, sentences: <span class="hljs-number">93.91</span>%P/<span class="hljs-number">92.62</span>%R/<span class="hljs-number">93.26</span>%
Epoch <span class="hljs-number">52</span>, logprob: <span class="hljs-number">-2.2911e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.76</span>%P/<span class="hljs-number">99.77</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">94.02</span>%P/<span class="hljs-number">92.49</span>%R/<span class="hljs-number">93.25</span>%
Epoch <span class="hljs-number">53</span>, logprob: <span class="hljs-number">-2.2195e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.74</span>%P/<span class="hljs-number">99.77</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">94.10</span>%P/<span class="hljs-number">93.74</span>%R/<span class="hljs-number">93.92</span>%
Epoch <span class="hljs-number">54</span>, logprob: <span class="hljs-number">-2.1996e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.78</span>%R/<span class="hljs-number">99.75</span>%, sentences: <span class="hljs-number">92.99</span>%P/<span class="hljs-number">92.99</span>%R/<span class="hljs-number">92.99</span>%
Epoch <span class="hljs-number">55</span>, logprob: <span class="hljs-number">-2.1129e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.74</span>%, sentences: <span class="hljs-number">93.12</span>%P/<span class="hljs-number">93.12</span>%R/<span class="hljs-number">93.12</span>%
Epoch <span class="hljs-number">56</span>, logprob: <span class="hljs-number">-2.2491e+003</span>, training acc: <span class="hljs-number">99.81</span>%, heldout tokens: <span class="hljs-number">99.68</span>%P/<span class="hljs-number">99.72</span>%R/<span class="hljs-number">99.70</span>%, sentences: <span class="hljs-number">93.61</span>%P/<span class="hljs-number">93.49</span>%R/<span class="hljs-number">93.55</span>%
Epoch <span class="hljs-number">57</span>, logprob: <span class="hljs-number">-2.1532e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.77</span>%P/<span class="hljs-number">99.76</span>%R/<span class="hljs-number">99.76</span>%, sentences: <span class="hljs-number">94.31</span>%P/<span class="hljs-number">93.37</span>%R/<span class="hljs-number">93.84</span>%
Epoch <span class="hljs-number">58</span>, logprob: <span class="hljs-number">-2.0931e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.75</span>%P/<span class="hljs-number">99.81</span>%R/<span class="hljs-number">99.78</span>%, sentences: <span class="hljs-number">93.57</span>%P/<span class="hljs-number">92.87</span>%R/<span class="hljs-number">93.22</span>%
Epoch <span class="hljs-number">59</span>, logprob: <span class="hljs-number">-2.1038e+003</span>, training acc: <span class="hljs-number">99.83</span>%, heldout tokens: <span class="hljs-number">99.70</span>%P/<span class="hljs-number">99.66</span>%R/<span class="hljs-number">99.68</span>%, sentences: <span class="hljs-number">93.23</span>%P/<span class="hljs-number">93.12</span>%R/<span class="hljs-number">93.17</span>%
Epoch <span class="hljs-number">60</span>, logprob: <span class="hljs-number">-2.2800e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.76</span>%P/<span class="hljs-number">99.82</span>%R/<span class="hljs-number">99.79</span>%, sentences: <span class="hljs-number">93.82</span>%P/<span class="hljs-number">93.12</span>%R/<span class="hljs-number">93.47</span>%
Epoch <span class="hljs-number">61</span>, logprob: <span class="hljs-number">-2.1557e+003</span>, training acc: <span class="hljs-number">99.82</span>%, heldout tokens: <span class="hljs-number">99.72</span>%P/<span class="hljs-number">99.75</span>%R/<span class="hljs-number">99.73</span>%, sentences: <span class="hljs-number">93.44</span>%P/<span class="hljs-number">92.74</span>%R/<span class="hljs-number">93.09</span>%
Stopping after <span class="hljs-number">30</span> iterations of not improving sum of sentence and token f1.
Choosing parameters from epoch <span class="hljs-number">30.</span>
Tagger model <span class="hljs-number">1</span> columns: lemma use=<span class="hljs-number">1</span>/provide=<span class="hljs-number">1</span>, xpostag use=<span class="hljs-number">0</span>/provide=<span class="hljs-number">0</span>, feats use=<span class="hljs-number">0</span>/provide=<span class="hljs-number">0</span>
Creating morphological dictionary <span class="hljs-keyword">for</span> tagger model <span class="hljs-number">1.</span>
Tagger model <span class="hljs-number">1</span> dictionary options: max_form_analyses=<span class="hljs-number">0</span>, custom dictionary_file=none
Tagger model <span class="hljs-number">1</span> guesser options: suffix_rules=<span class="hljs-number">8</span>, prefixes_max=<span class="hljs-number">4</span>, prefix_min_count=<span class="hljs-number">10</span>, enrich_dictionary=<span class="hljs-number">4</span>
Tagger model <span class="hljs-number">1</span> options: iterations=<span class="hljs-number">20</span>, early_stopping=<span class="hljs-number">1</span>, templates=lemmatizer
Training tagger model <span class="hljs-number">1.</span>
Iteration <span class="hljs-number">1</span>: done, accuracy <span class="hljs-number">85.97</span>%, heldout accuracy <span class="hljs-number">90.79</span>%t/<span class="hljs-number">95.47</span>%l/<span class="hljs-number">87.59</span>%b
Iteration <span class="hljs-number">2</span>: done, accuracy <span class="hljs-number">93.36</span>%, heldout accuracy <span class="hljs-number">91.26</span>%t/<span class="hljs-number">96.16</span>%l/<span class="hljs-number">88.64</span>%b
Iteration <span class="hljs-number">3</span>: done, accuracy <span class="hljs-number">94.85</span>%, heldout accuracy <span class="hljs-number">91.30</span>%t/<span class="hljs-number">96.23</span>%l/<span class="hljs-number">88.72</span>%b
Iteration <span class="hljs-number">4</span>: done, accuracy <span class="hljs-number">95.78</span>%, heldout accuracy <span class="hljs-number">91.39</span>%t/<span class="hljs-number">96.24</span>%l/<span class="hljs-number">88.82</span>%b
Iteration <span class="hljs-number">5</span>: done, accuracy <span class="hljs-number">96.46</span>%, heldout accuracy <span class="hljs-number">91.53</span>%t/<span class="hljs-number">96.30</span>%l/<span class="hljs-number">89.02</span>%b
Iteration <span class="hljs-number">6</span>: done, accuracy <span class="hljs-number">96.78</span>%, heldout accuracy <span class="hljs-number">91.66</span>%t/<span class="hljs-number">96.28</span>%l/<span class="hljs-number">89.09</span>%b
Iteration <span class="hljs-number">7</span>: done, accuracy <span class="hljs-number">97.27</span>%, heldout accuracy <span class="hljs-number">91.73</span>%t/<span class="hljs-number">96.27</span>%l/<span class="hljs-number">89.11</span>%b
Iteration <span class="hljs-number">8</span>: done, accuracy <span class="hljs-number">97.55</span>%, heldout accuracy <span class="hljs-number">91.76</span>%t/<span class="hljs-number">96.24</span>%l/<span class="hljs-number">89.12</span>%b
Iteration <span class="hljs-number">9</span>: done, accuracy <span class="hljs-number">97.89</span>%, heldout accuracy <span class="hljs-number">91.81</span>%t/<span class="hljs-number">96.30</span>%l/<span class="hljs-number">89.18</span>%b
Iteration <span class="hljs-number">10</span>: done, accuracy <span class="hljs-number">98.05</span>%, heldout accuracy <span class="hljs-number">91.75</span>%t/<span class="hljs-number">96.24</span>%l/<span class="hljs-number">89.10</span>%b
Iteration <span class="hljs-number">11</span>: done, accuracy <span class="hljs-number">98.29</span>%, heldout accuracy <span class="hljs-number">91.73</span>%t/<span class="hljs-number">96.20</span>%l/<span class="hljs-number">89.06</span>%b
Iteration <span class="hljs-number">12</span>: done, accuracy <span class="hljs-number">98.41</span>%, heldout accuracy <span class="hljs-number">91.72</span>%t/<span class="hljs-number">96.14</span>%l/<span class="hljs-number">89.01</span>%b
Iteration <span class="hljs-number">13</span>: done, accuracy <span class="hljs-number">98.46</span>%, heldout accuracy <span class="hljs-number">91.73</span>%t/<span class="hljs-number">96.10</span>%l/<span class="hljs-number">88.98</span>%b
Iteration <span class="hljs-number">14</span>: done, accuracy <span class="hljs-number">98.67</span>%, heldout accuracy <span class="hljs-number">91.73</span>%t/<span class="hljs-number">96.12</span>%l/<span class="hljs-number">88.96</span>%b
Iteration <span class="hljs-number">15</span>: done, accuracy <span class="hljs-number">98.71</span>%, heldout accuracy <span class="hljs-number">91.74</span>%t/<span class="hljs-number">96.11</span>%l/<span class="hljs-number">88.97</span>%b
Iteration <span class="hljs-number">16</span>: done, accuracy <span class="hljs-number">98.84</span>%, heldout accuracy <span class="hljs-number">91.74</span>%t/<span class="hljs-number">96.12</span>%l/<span class="hljs-number">88.99</span>%b
Iteration <span class="hljs-number">17</span>: done, accuracy <span class="hljs-number">98.85</span>%, heldout accuracy <span class="hljs-number">91.74</span>%t/<span class="hljs-number">96.09</span>%l/<span class="hljs-number">88.96</span>%b
Iteration <span class="hljs-number">18</span>: done, accuracy <span class="hljs-number">98.90</span>%, heldout accuracy <span class="hljs-number">91.76</span>%t/<span class="hljs-number">96.04</span>%l/<span class="hljs-number">88.96</span>%b
Iteration <span class="hljs-number">19</span>: done, accuracy <span class="hljs-number">99.05</span>%, heldout accuracy <span class="hljs-number">91.73</span>%t/<span class="hljs-number">96.03</span>%l/<span class="hljs-number">88.92</span>%b
Iteration <span class="hljs-number">20</span>: done, accuracy <span class="hljs-number">99.14</span>%, heldout accuracy <span class="hljs-number">91.70</span>%t/<span class="hljs-number">95.99</span>%l/<span class="hljs-number">88.84</span>%b
Chosen tagger model from iteration <span class="hljs-number">9</span>
Tagger model <span class="hljs-number">2</span> columns: lemma use=<span class="hljs-number">1</span>/provide=<span class="hljs-number">0</span>, xpostag use=<span class="hljs-number">1</span>/provide=<span class="hljs-number">1</span>, feats use=<span class="hljs-number">1</span>/provide=<span class="hljs-number">1</span>
Creating morphological dictionary <span class="hljs-keyword">for</span> tagger model <span class="hljs-number">2.</span>
Tagger model <span class="hljs-number">2</span> dictionary options: max_form_analyses=<span class="hljs-number">0</span>, custom dictionary_file=none
Tagger model <span class="hljs-number">2</span> guesser options: suffix_rules=<span class="hljs-number">8</span>, prefixes_max=<span class="hljs-number">0</span>, prefix_min_count=<span class="hljs-number">10</span>, enrich_dictionary=<span class="hljs-number">4</span>
Tagger model <span class="hljs-number">2</span> options: iterations=<span class="hljs-number">20</span>, early_stopping=<span class="hljs-number">1</span>, templates=tagger
Training tagger model <span class="hljs-number">2.</span>
Iteration <span class="hljs-number">1</span>: done, accuracy <span class="hljs-number">80.17</span>%, heldout accuracy <span class="hljs-number">60.42</span>%t/<span class="hljs-number">92.10</span>%l/<span class="hljs-number">58.61</span>%b
Iteration <span class="hljs-number">2</span>: done, accuracy <span class="hljs-number">89.15</span>%, heldout accuracy <span class="hljs-number">61.48</span>%t/<span class="hljs-number">92.47</span>%l/<span class="hljs-number">59.71</span>%b
Iteration <span class="hljs-number">3</span>: done, accuracy <span class="hljs-number">92.28</span>%, heldout accuracy <span class="hljs-number">61.88</span>%t/<span class="hljs-number">92.54</span>%l/<span class="hljs-number">60.13</span>%b
Iteration <span class="hljs-number">4</span>: done, accuracy <span class="hljs-number">94.10</span>%, heldout accuracy <span class="hljs-number">61.90</span>%t/<span class="hljs-number">92.54</span>%l/<span class="hljs-number">60.19</span>%b
Iteration <span class="hljs-number">5</span>: done, accuracy <span class="hljs-number">95.40</span>%, heldout accuracy <span class="hljs-number">62.22</span>%t/<span class="hljs-number">92.54</span>%l/<span class="hljs-number">60.53</span>%b
Iteration <span class="hljs-number">6</span>: done, accuracy <span class="hljs-number">96.25</span>%, heldout accuracy <span class="hljs-number">62.24</span>%t/<span class="hljs-number">92.47</span>%l/<span class="hljs-number">60.56</span>%b
Iteration <span class="hljs-number">7</span>: done, accuracy <span class="hljs-number">96.95</span>%, heldout accuracy <span class="hljs-number">62.33</span>%t/<span class="hljs-number">92.47</span>%l/<span class="hljs-number">60.63</span>%b
Iteration <span class="hljs-number">8</span>: done, accuracy <span class="hljs-number">97.47</span>%, heldout accuracy <span class="hljs-number">62.45</span>%t/<span class="hljs-number">92.53</span>%l/<span class="hljs-number">60.76</span>%b
Iteration <span class="hljs-number">9</span>: done, accuracy <span class="hljs-number">97.76</span>%, heldout accuracy <span class="hljs-number">62.39</span>%t/<span class="hljs-number">92.48</span>%l/<span class="hljs-number">60.70</span>%b
Iteration <span class="hljs-number">10</span>: done, accuracy <span class="hljs-number">98.19</span>%, heldout accuracy <span class="hljs-number">62.57</span>%t/<span class="hljs-number">92.50</span>%l/<span class="hljs-number">60.88</span>%b
Iteration <span class="hljs-number">11</span>: done, accuracy <span class="hljs-number">98.34</span>%, heldout accuracy <span class="hljs-number">62.58</span>%t/<span class="hljs-number">92.50</span>%l/<span class="hljs-number">60.90</span>%b
Iteration <span class="hljs-number">12</span>: done, accuracy <span class="hljs-number">98.56</span>%, heldout accuracy <span class="hljs-number">62.59</span>%t/<span class="hljs-number">92.47</span>%l/<span class="hljs-number">60.91</span>%b
Iteration <span class="hljs-number">13</span>: done, accuracy <span class="hljs-number">98.71</span>%, heldout accuracy <span class="hljs-number">62.66</span>%t/<span class="hljs-number">92.45</span>%l/<span class="hljs-number">60.95</span>%b
Iteration <span class="hljs-number">14</span>: done, accuracy <span class="hljs-number">98.87</span>%, heldout accuracy <span class="hljs-number">62.68</span>%t/<span class="hljs-number">92.43</span>%l/<span class="hljs-number">60.96</span>%b
Iteration <span class="hljs-number">15</span>: done, accuracy <span class="hljs-number">98.98</span>%, heldout accuracy <span class="hljs-number">62.73</span>%t/<span class="hljs-number">92.46</span>%l/<span class="hljs-number">61.02</span>%b
Iteration <span class="hljs-number">16</span>: done, accuracy <span class="hljs-number">99.12</span>%, heldout accuracy <span class="hljs-number">62.75</span>%t/<span class="hljs-number">92.46</span>%l/<span class="hljs-number">61.05</span>%b
Iteration <span class="hljs-number">17</span>: done, accuracy <span class="hljs-number">99.18</span>%, heldout accuracy <span class="hljs-number">62.75</span>%t/<span class="hljs-number">92.42</span>%l/<span class="hljs-number">61.04</span>%b
Iteration <span class="hljs-number">18</span>: done, accuracy <span class="hljs-number">99.27</span>%, heldout accuracy <span class="hljs-number">62.89</span>%t/<span class="hljs-number">92.46</span>%l/<span class="hljs-number">61.16</span>%b
Iteration <span class="hljs-number">19</span>: done, accuracy <span class="hljs-number">99.30</span>%, heldout accuracy <span class="hljs-number">62.85</span>%t/<span class="hljs-number">92.45</span>%l/<span class="hljs-number">61.15</span>%b
Iteration <span class="hljs-number">20</span>: done, accuracy <span class="hljs-number">99.33</span>%, heldout accuracy <span class="hljs-number">62.86</span>%t/<span class="hljs-number">92.47</span>%l/<span class="hljs-number">61.16</span>%b
Chosen tagger model from iteration <span class="hljs-number">18</span>
Parser transition options: <span class="hljs-keyword">system</span>=projective, oracle=dynamic, structured_interval=<span class="hljs-number">8</span>, single_root=<span class="hljs-number">1</span>
Parser uses lemmas/upos/xpos/feats: automatically generated by tagger
Parser embeddings options: upostag=<span class="hljs-number">20</span>, feats=<span class="hljs-number">20</span>, xpostag=<span class="hljs-number">0</span>, form=<span class="hljs-number">50</span>, lemma=<span class="hljs-number">0</span>, deprel=<span class="hljs-number">20</span>
  form mincount=<span class="hljs-number">2</span>, precomputed form embeddings=wordvectors.vec
  lemma mincount=<span class="hljs-number">2</span>, precomputed lemma embeddings=none
Parser network options: iterations=<span class="hljs-number">30</span>, hidden_layer=<span class="hljs-number">200</span>, batch_size=<span class="hljs-number">10</span>,
  learning_rate=<span class="hljs-number">0.0100</span>, learning_rate_final=<span class="hljs-number">0.0010</span>, l2=<span class="hljs-number">0.5000</span>, early_stopping=<span class="hljs-number">1</span>
Initialized <span class="hljs-string">'universal_tag'</span> embedding with <span class="hljs-number">0</span>,<span class="hljs-number">16</span> words and <span class="hljs-number">0.0</span>%,<span class="hljs-number">100.0</span>% coverage.
Initialized <span class="hljs-string">'feats'</span> embedding with <span class="hljs-number">0</span>,<span class="hljs-number">328</span> words and <span class="hljs-number">0.0</span>%,<span class="hljs-number">100.0</span>% coverage.
Initialized <span class="hljs-string">'form'</span> embedding with <span class="hljs-number">15101</span>,<span class="hljs-number">15303</span> words and <span class="hljs-number">74.1</span>%,<span class="hljs-number">87.6</span>% coverage.
Initialized <span class="hljs-string">'deprel'</span> embedding with <span class="hljs-number">0</span>,<span class="hljs-number">42</span> words and <span class="hljs-number">0.0</span>%,<span class="hljs-number">100.0</span>% coverage.
Iteration <span class="hljs-number">1</span>: training logprob <span class="hljs-number">-2.6984e+005</span>, heldout UAS <span class="hljs-number">73.54</span>%, LAS <span class="hljs-number">65.82</span>%
Iteration <span class="hljs-number">2</span>: training logprob <span class="hljs-number">-4.5160e+005</span>, heldout UAS <span class="hljs-number">68.76</span>%, LAS <span class="hljs-number">61.06</span>%
Iteration <span class="hljs-number">3</span>: training logprob <span class="hljs-number">-4.0263e+005</span>, heldout UAS <span class="hljs-number">72.26</span>%, LAS <span class="hljs-number">64.58</span>%
Iteration <span class="hljs-number">4</span>: training logprob <span class="hljs-number">-3.7276e+005</span>, heldout UAS <span class="hljs-number">72.59</span>%, LAS <span class="hljs-number">65.67</span>%
Iteration <span class="hljs-number">5</span>: training logprob <span class="hljs-number">-3.5034e+005</span>, heldout UAS <span class="hljs-number">73.51</span>%, LAS <span class="hljs-number">66.70</span>%
Iteration <span class="hljs-number">6</span>: training logprob <span class="hljs-number">-3.3267e+005</span>, heldout UAS <span class="hljs-number">73.25</span>%, LAS <span class="hljs-number">65.88</span>%
Iteration <span class="hljs-number">7</span>: training logprob <span class="hljs-number">-3.1554e+005</span>, heldout UAS <span class="hljs-number">75.35</span>%, LAS <span class="hljs-number">68.21</span>%
Iteration <span class="hljs-number">8</span>: training logprob <span class="hljs-number">-2.9980e+005</span>, heldout UAS <span class="hljs-number">76.47</span>%, LAS <span class="hljs-number">69.86</span>%
Iteration <span class="hljs-number">9</span>: training logprob training logprob <span class="hljs-number">-2.8442e+005</span>, heldout UAS <span class="hljs-number">75.99</span>%, LAS <span class="hljs-number">69.81</span>%
Iteration <span class="hljs-number">10</span>: training logprob training logprob <span class="hljs-number">-2.7191e+005</span>, heldout UAS <span class="hljs-number">77.95</span>%, LAS <span class="hljs-number">71.58</span>%
Iteration <span class="hljs-number">11</span>: training logprob <span class="hljs-number">-2.5799e+005</span>, heldout UAS <span class="hljs-number">77.02</span>%, LAS <span class="hljs-number">70.56</span>%
Iteration <span class="hljs-number">12</span>: training logprob training logprob <span class="hljs-number">-2.5025e+005</span>, heldout UAS <span class="hljs-number">77.68</span>%, LAS <span class="hljs-number">71.32</span>%
Iteration <span class="hljs-number">13</span>: training logprob <span class="hljs-number">-2.3856e+005</span>, heldout UAS <span class="hljs-number">77.94</span>%, LAS <span class="hljs-number">72.20</span>%
Iteration <span class="hljs-number">14</span>: training logprob <span class="hljs-number">-2.3122e+005</span>training logprob <span class="hljs-number">-2.5025e+005</span>, heldout UAS <span class="hljs-number">77.68</span>%, LAS <span class="hljs-number">71.32</span>%
Iteration <span class="hljs-number">13</span>: training logprob <span class="hljs-number">-2.3856e+005</span>, heldout UAS <span class="hljs-number">77.94</span>%, LAS <span class="hljs-number">72.20</span>%
Iteration <span class="hljs-number">14</span>: training logprob <span class="hljs-number">-2.3122e+005</span>, heldout UAS <span class="hljs-number">77.60</span>%, LAS <span class="hljs-number">71.27</span>%
Iteration <span class="hljs-number">15</span>: training logprob training logprob <span class="hljs-number">-2.5025e+005</span>, heldout UAS <span class="hljs-number">77.68</span>%, LAS <span class="hljs-number">71.32</span>%
Iteration <span class="hljs-number">13</span>: training logprob <span class="hljs-number">-2.3856e+005</span>, heldout UAS <span class="hljs-number">77.94</span>%, LAS <span class="hljs-number">72.20</span>%
Iteration <span class="hljs-number">14</span>: training logprob <span class="hljs-number">-2.3122e+005</span>, heldout UAS <span class="hljs-number">77.60</span>%, LAS <span class="hljs-number">71.27</span>%
Iteration <span class="hljs-number">15</span>: training logprob <span class="hljs-number">-2.2353e+005</span>, heldout UAS <span class="hljs-number">77.40</span>%, LAS <span class="hljs-number">71.45</span>%
Iteration <span class="hljs-number">16</span>: training logprob <span class="hljs-number">-2.1468e+005</span>, heldout UAS <span class="hljs-number">79.22</span>%, LAS <span class="hljs-number">72.95</span>%
Iteration <span class="hljs-number">17</span>: training logprob <span class="hljs-number">-2.0689e+005</span>, heldout UAS <span class="hljs-number">79.31</span>%, LAS <span class="hljs-number">73.19</span>%
Iteration <span class="hljs-number">18</span>: training logprob <span class="hljs-number">-2.0281e+005</span>, heldout UAS <span class="hljs-number">78.96</span>%, LAS <span class="hljs-number">72.91</span>%
Iteration <span class="hljs-number">19</span>: training logprob <span class="hljs-number">-1.9567e+005</span>, heldout UAS <span class="hljs-number">79.10</span>%, LAS <span class="hljs-number">73.27</span>%
Iteration <span class="hljs-number">20</span>: training logprob <span class="hljs-number">-1.9331e+005</span>, heldout UAS <span class="hljs-number">79.86</span>%, LAS <span class="hljs-number">74.23</span>%
Iteration <span class="hljs-number">21</span>: training logprob <span class="hljs-number">-1.8678e+005</span>, heldout UAS <span class="hljs-number">80.10</span>%, LAS <span class="hljs-number">74.18</span>%
Iteration <span class="hljs-number">22</span>: training logprob <span class="hljs-number">-1.8485e+005</span>, heldout UAS <span class="hljs-number">79.65</span>%, LAS <span class="hljs-number">73.87</span>%
Iteration <span class="hljs-number">23</span>: training logprob <span class="hljs-number">-1.8144e+005</span>, heldout UAS <span class="hljs-number">79.15</span>%, LAS <span class="hljs-number">73.04</span>%
Iteration <span class="hljs-number">24</span>: training logprob <span class="hljs-number">-1.7638e+005</span>, heldout UAS <span class="hljs-number">79.22</span>%, LAS <span class="hljs-number">73.43</span>%
Iteration <span class="hljs-number">25</span>: training logprob <span class="hljs-number">-1.7619e+005</span>, heldout UAS <span class="hljs-number">79.86</span>%, LAS <span class="hljs-number">73.97</span>%
Iteration <span class="hljs-number">26</span>: training logprob <span class="hljs-number">-1.7434e+005</span>, heldout UAS <span class="hljs-number">79.97</span>%, LAS <span class="hljs-number">74.50</span>%
Iteration <span class="hljs-number">27</span>: training logprob <span class="hljs-number">-1.6983e+005</span>, heldout UAS <span class="hljs-number">80.18</span>%, LAS <span class="hljs-number">74.40</span>%
Iteration <span class="hljs-number">28</span>: training logprob <span class="hljs-number">-1.6875e+005</span>, heldout UAS <span class="hljs-number">80.53</span>%, LAS <span class="hljs-number">74.58</span>%
Iteration <span class="hljs-number">29</span>: training logprob <span class="hljs-number">-1.6827e+005</span>, heldout UAS <span class="hljs-number">79.67</span>%, LAS <span class="hljs-number">73.95</span>%
Iteration <span class="hljs-number">30</span>: training logprob <span class="hljs-number">-1.6893e+005</span>, heldout UAS <span class="hljs-number">80.22</span>%, LAS <span class="hljs-number">74.60</span>%
Using early stopping -- choosing network from iteration <span class="hljs-number">30</span>
&gt; <span class="hljs-keyword">print</span>(Sys.time())
[<span class="hljs-number">1</span>] <span class="hljs-string">"2020-09-25 21:40:46 CEST"</span>

&gt; ## Evaluate the accuracy
&gt; m &lt;- udpipe_load_model(<span class="hljs-string">"de_gsd-ud-2.6-20200924.udpipe"</span>)
&gt; goodness_of_fit &lt;- udpipe_accuracy(m, <span class="hljs-string">"test.conllu"</span>, tokenizer = <span class="hljs-string">"default"</span>, tagger = <span class="hljs-string">"default"</span>, parser = <span class="hljs-string">"default"</span>)
&gt; cat(goodness_of_fit$accuracy, sep = <span class="hljs-string">"\n"</span>) 
Number of SpaceAfter=No features <span class="hljs-keyword">in</span> gold data: <span class="hljs-number">2423</span>
Tokenizer tokens - <span class="hljs-keyword">system</span>: <span class="hljs-number">16230</span>, gold: <span class="hljs-number">16224</span>, precision: <span class="hljs-number">99.45</span>%, recall: <span class="hljs-number">99.49</span>%, f1: <span class="hljs-number">99.47</span>%
Tokenizer multiword tokens - <span class="hljs-keyword">system</span>: <span class="hljs-number">274</span>, gold: <span class="hljs-number">274</span>, precision: <span class="hljs-number">100.00</span>%, recall: <span class="hljs-number">100.00</span>%, f1: <span class="hljs-number">100.00</span>%
Tokenizer words - <span class="hljs-keyword">system</span>: <span class="hljs-number">16504</span>, gold: <span class="hljs-number">16498</span>, precision: <span class="hljs-number">99.46</span>%, recall: <span class="hljs-number">99.50</span>%, f1: <span class="hljs-number">99.48</span>%
Tokenizer sentences - <span class="hljs-keyword">system</span>: <span class="hljs-number">913</span>, gold: <span class="hljs-number">977</span>, precision: <span class="hljs-number">83.24</span>%, recall: <span class="hljs-number">77.79</span>%, f1: <span class="hljs-number">80.42</span>%
Tagging from plain <span class="hljs-keyword">text</span> (CoNLL17 F1 score) - gold forms: <span class="hljs-number">16498</span>, upostag: <span class="hljs-number">91.55</span>%, xpostag: <span class="hljs-number">79.48</span>%, feats: <span class="hljs-number">69.74</span>%, alltags: <span class="hljs-number">62.84</span>%, lemmas: <span class="hljs-number">95.29</span>%
Parsing from plain <span class="hljs-keyword">text</span> with computed tags (CoNLL17 F1 score) - gold forms: <span class="hljs-number">16498</span>, UAS: <span class="hljs-number">77.10</span>%, LAS: <span class="hljs-number">71.67</span>%
&gt; goodness_of_fit &lt;- udpipe_accuracy(m, <span class="hljs-string">"test.conllu"</span>, tokenizer = <span class="hljs-string">"none"</span>, tagger = <span class="hljs-string">"default"</span>, parser = <span class="hljs-string">"default"</span>)
&gt; cat(goodness_of_fit$accuracy, sep = <span class="hljs-string">"\n"</span>) 
Tagging from gold tokenization - forms: <span class="hljs-number">16498</span>, upostag: <span class="hljs-number">92.14</span>%, xpostag: <span class="hljs-number">79.80</span>%, feats: <span class="hljs-number">70.18</span>%, alltags: <span class="hljs-number">63.35</span>%, lemmas: <span class="hljs-number">95.78</span>%
Parsing from gold tokenization with computed tags - forms: <span class="hljs-number">16498</span>, UAS: <span class="hljs-number">79.88</span>%, LAS: <span class="hljs-number">74.22</span>%
&gt; goodness_of_fit &lt;- udpipe_accuracy(m, <span class="hljs-string">"test.conllu"</span>, tokenizer = <span class="hljs-string">"none"</span>, tagger = <span class="hljs-string">"none"</span>, parser = <span class="hljs-string">"default"</span>)
&gt; cat(goodness_of_fit$accuracy, sep = <span class="hljs-string">"\n"</span>) 
Parsing from gold tokenization with gold tags - forms: <span class="hljs-number">16498</span>, UAS: <span class="hljs-number">84.24</span>%, LAS: <span class="hljs-number">79.77</span>%
</code></pre>
<p>Which gives accuracy statistics similar to the officially released UDPipe models for this treebank.
Note that model training takes a while depending on the size of the treebank and your hyperparameter settings. This example was run on a Windows i5 CPU laptop with 1.7Ghz, so no GPU needed.
Good luck!</p>
<h2><a class="anchor" aria-hidden="true" name="support-in-text-mining"></a><a href="#support-in-text-mining" aria-hidden="true" class="hash-link" ><svg aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Support in text mining</h2>
<p>Need support in text mining.
Contact BNOSAC: <a href="http://www.bnosac.be">http://www.bnosac.be</a></p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="doc2.html">← Text Annotation</a><a class="docs-next button" href="doc8.html">Parallel Annotation →</a></div></div></div></div><footer class="nav-footer" id="footer"><section class="sitemap"><a href="/udpipe/" class="nav-home"><img src="/udpipe/img/logo-udpipe-r.png" alt="NLP with R and UDPipe" width="66" height="58"/></a><div><h5>Docs</h5><a href="/udpipe/docs/doc0.html">Getting Started</a><a href="/udpipe/docs/doc1.html">Introduction</a><a href="/udpipe/docs/doc2.html">Natural Language Processing</a></div><div><h5>Community</h5><a href="/udpipe/en/users.html">User Showcase</a><a href="https://github.com/bnosac/udpipe/issues">Report issue</a></div><div><h5>More</h5><a href="/udpipe/blog">Blog</a><a href="https://github.com/bnosac/udpipe">GitHub</a><a class="github-button" href="https://github.com/bnosac/udpipe" data-icon="octicon-star" data-count-href="/bnosac/udpipe/stargazers" data-show-count="true" data-count-aria-label="# stargazers on GitHub" aria-label="Star this project on GitHub">Star</a></div></section><a href="http://www.bnosac.be" target="_blank" class="fbOpenSource"><img src="/udpipe/img/logo-bnosac.png" alt="BNOSAC Open Source" width="170" height="45"/></a><section class="copyright">Copyright © 2020 BNOSAC</section></footer></div></body></html>